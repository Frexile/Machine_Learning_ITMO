{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Text_processing.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "5QYTwyMtWhAZ",
        "DbJrUpARWhAd",
        "MI18l-l9WhAk",
        "1wrEGqBSWhAr",
        "gStgBJy2WhAx"
      ],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.5.2"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Frexile/Machine_Learning_ITMO/blob/main/Text_processing.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kz1hqb2oWg96"
      },
      "source": [
        "# Инструменты для работы с текстом"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iXhKw6r2Wg97"
      },
      "source": [
        "Анализ текстовых данных - это отдельное направление, здесь будет совсем небольшое введение.\n",
        "С текстовыми данными можно решать как задачи обучения с учителем (классификация текстов), так и задачу обучения без учителя (кластеризация).\n",
        "\n",
        "Предобработка текста\n",
        "\n",
        "Первый шаг любой аналитики – получение данных. Предположим, что данные представляются собой набор текстов. Все известные нам алгоритмы работают не в текстами, а с объектами, которые описываются вектором признаков (чаще всего численных, категориальные мы умеем преобразовывать). Что делать, если наши объекты - это текст? \n",
        "\n",
        "Следующая после получения данных задача: предобработка. Основная цель предобработки: преобразовать текстовые данные в удобный для построения модели вид.\n",
        "\n",
        "Базовые шаги предобработки:\n",
        "1. токенизация\n",
        "2. приведение к нижнему регистру\n",
        "3. удаление стоп-слов\n",
        "4. удаление пунктуации\n",
        "5. фильтрация по частоте/длине/соответствию регулярному выражению\n",
        "6. лемматизация или стемминг\n",
        "7. векторизация (эмбеддинг)\n",
        "\n",
        "Чаще всего применяются все эти шаги, но в разных задачах какие-то могут опускаться, поскольку приводят к потере информации"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IX-AeH8RWg9-"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "from sklearn.metrics import * \n",
        "from sklearn.model_selection import train_test_split "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zpq4QOU5Wg-H"
      },
      "source": [
        "* Сейчас мы попробуем получить преобразование предложений в численный вектор, с которым может работать стандартный алгоритм машинного обучения. \n",
        "* Для этого нам понадобится познакомиться с понятием n-gram - самых мелких элементов предложения, с которыми можно работать. \n",
        "* Подсчитав количество этих n-грам в предложениях, мы получим искомые численные представления."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i_7DyyXRWg-K"
      },
      "source": [
        "## n-граммы\n",
        "\n",
        "Самые мелкие структуры языка, с которыми мы работаем, называются **n-граммами**.\n",
        "У n-граммы есть параметр n - количество слов, которые попадают в такое представление текста.\n",
        "* Если n = 1 - то мы смотрим на то, сколько раз каждое слово встретилось в тексте. Получаем _униграммы_\n",
        "* Если n = 2 - то мы смотрим на то, сколько раз каждая пара подряд идущих слов, встретилась в тексте. Получаем _биграммы_"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "quiUoyqNb3WA"
      },
      "source": [
        "Функция для работы с n-граммами реализована в библиотке **nltk** (Natural Language ToolKit), импортируем эту функцию: "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hcrWxBzzWg-K"
      },
      "source": [
        "from nltk import ngrams"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7ib-zYTvfQq5"
      },
      "source": [
        "Прежде чем получать n-граммы, нужно разделить предложение на отдельные слова.  Для этого используем метод ```split()```."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wJ9aYx2UPK44"
      },
      "source": [
        "sentence = 'Кто же победит на выборах в США: Трамп или Байден???!'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A2Ql8Em4Wg-N",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "952b535a-9e7a-49b7-e7f5-95042114339a"
      },
      "source": [
        "sentence_split = sentence.split()\n",
        "sentence_split"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Кто',\n",
              " 'же',\n",
              " 'победит',\n",
              " 'на',\n",
              " 'выборах',\n",
              " 'в',\n",
              " 'США:',\n",
              " 'Трамп',\n",
              " 'или',\n",
              " 'Байден???!']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 95
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GVIvz7OLN8yV"
      },
      "source": [
        "Кажется, что нам тут мешают знаки препинания. Дайвайте от них избавимся. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3i3IvCTPOIfB",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "1d0098d0-2778-4251-89e7-e98a6b7e4a36"
      },
      "source": [
        "import string\n",
        "string.punctuation"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 96
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NTiPD_lfPERo"
      },
      "source": [
        "for ch in string.punctuation:\n",
        "  sentence = sentence.replace(ch,\"\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4eY7anMcPxGB",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "1c8a4857-c568-4b41-f2d2-8147dff1a4fe"
      },
      "source": [
        "sentence"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'Кто же победит на выборах в США Трамп или Байден'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 98
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AWvXpEdDP4kI"
      },
      "source": [
        "sentence_split = sentence.split()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8uy1TrfAP-ui",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "051ca728-f2b8-4f16-d253-25f90f04d43c"
      },
      "source": [
        "sentence_split"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Кто', 'же', 'победит', 'на', 'выборах', 'в', 'США', 'Трамп', 'или', 'Байден']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d6V5P2Jcc4Oy"
      },
      "source": [
        "Чтобы получить n-грамму для такой последовательности, используем функцию ```ngrams()```. \n",
        "\n",
        "На вход передается два параметра:\n",
        "* лист с разделенным на отдельные слова предложением (у нас он хранится в переменной ```sent```);\n",
        "* параметр n, определяющий, какой тип n-грамм мы хотим получить.\n",
        "\n",
        "\n",
        "Чтобы полученный объект отобразить, делаем из него ```list```. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F9oqpykUc5e9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a79d4428-a886-4c48-9e01-e3a658f41e07"
      },
      "source": [
        "list(ngrams(sentence_split, 1)) # униграммы"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('Кто',),\n",
              " ('же',),\n",
              " ('победит',),\n",
              " ('на',),\n",
              " ('выборах',),\n",
              " ('в',),\n",
              " ('США',),\n",
              " ('Трамп',),\n",
              " ('или',),\n",
              " ('Байден',)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GZKRhRlxfoj4"
      },
      "source": [
        "Аналогично мы можем получить биграммы - для этого заменяем параметр **n** в функции **ngrams** с 1 на 2."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bzl6t5dpWg-P",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "833edaab-2850-4a7a-b493-1adebdd37c1c"
      },
      "source": [
        "list(ngrams(sentence_split, 2)) # биграммы"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('Кто', 'же'),\n",
              " ('же', 'победит'),\n",
              " ('победит', 'на'),\n",
              " ('на', 'выборах'),\n",
              " ('выборах', 'в'),\n",
              " ('в', 'США'),\n",
              " ('США', 'Трамп'),\n",
              " ('Трамп', 'или'),\n",
              " ('или', 'Байден')]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nCkkFzWLWg-R",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "883006fe-2689-4f7e-c0a3-70e0a4f9ddb8"
      },
      "source": [
        "list(ngrams(sentence_split, 3)) # триграммы"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('Кто', 'же', 'победит'),\n",
              " ('же', 'победит', 'на'),\n",
              " ('победит', 'на', 'выборах'),\n",
              " ('на', 'выборах', 'в'),\n",
              " ('выборах', 'в', 'США'),\n",
              " ('в', 'США', 'Трамп'),\n",
              " ('США', 'Трамп', 'или'),\n",
              " ('Трамп', 'или', 'Байден')]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GygS6_fJWg-S",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2102413e-cb48-418c-f855-a75ebcaad5da"
      },
      "source": [
        "list(ngrams(sentence_split, 5)) # ... пентаграммы"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('Кто', 'же', 'победит', 'на', 'выборах'),\n",
              " ('же', 'победит', 'на', 'выборах', 'в'),\n",
              " ('победит', 'на', 'выборах', 'в', 'США'),\n",
              " ('на', 'выборах', 'в', 'США', 'Трамп'),\n",
              " ('выборах', 'в', 'США', 'Трамп', 'или'),\n",
              " ('в', 'США', 'Трамп', 'или', 'Байден')]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_JewKs4XU-so"
      },
      "source": [
        "## Векторизаторы\n",
        "\n",
        "Векторизатор преобразует слово или набор слов в числовой вектор, понятный алгоритму машинного обучения, который привык работать с числовыми табличными данными.\n",
        "\n",
        "Ниже - пример преобразования слов в двумерных вектор, каждому слову соответствует точка на плоскости.\n",
        "\n",
        "<a href=\"https://drive.google.com/uc?id=1ukv-FTj0jeVdcgVlOaNBocUfNuYGGVZg\n",
        "\" target=\"_blank\"><img src=\"https://drive.google.com/uc?id=1ukv-FTj0jeVdcgVlOaNBocUfNuYGGVZg\" \n",
        "alt=\"IMAGE ALT TEXT HERE\" width=\"600\" border=\"0\" /></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V5hiNv2eVAc-"
      },
      "source": [
        "На начальном этапе нам будет достаточно тех инструментов, которые уже есть в библиотеке **sklearn**."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cPplZnxeVEBR"
      },
      "source": [
        "from sklearn.tree import DecisionTreeClassifier # можно заменить на другой классификатор\n",
        "from sklearn.naive_bayes import MultinomialNB # наивный байесовский классификатор\n",
        "from sklearn.feature_extraction.text import CountVectorizer # модель \"мешка слов\", см. далее"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eBN16KYZWg-U"
      },
      "source": [
        "Самый простой способ извлечь признаки из текстовых данных -- векторизаторы: `CountVectorizer` и `TfidfVectorizer`\n",
        "\n",
        "Объект `CountVectorizer` делает следующую вещь:\n",
        "* строит для каждого документа (каждой пришедшей ему строки) вектор размерности `n`, где `n` -- количество слов или n-грам во всём корпусе\n",
        "* заполняет каждый i-тый элемент количеством вхождений слова в данный документ\n",
        "\n",
        "<a href=\"https://drive.google.com/uc?id=1ukv-FTj0jeVdcgVlOaNBocUfNuYGGVZg\n",
        "\" target=\"_blank\"><img src=\"https://drive.google.com/uc?id=1jHmkrGZTMawM46Yzxh243Ur1y5pYKzrl\" \n",
        "alt=\"IMAGE ALT TEXT HERE\" width=\"600\" border=\"0\" /></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oklbwY_vWg-X"
      },
      "source": [
        "На рисунке пример векторизации для униграмм, но можно использовать любые n-граммы. Для этого у объекта ```CountVectorizer()``` есть параметр **ngram_range**, который отвечает за то, какие n-граммы мы используем в качестве признаов:<br/>\n",
        "ngram_range=(1, 1) -- униграммы<br/>\n",
        "ngram_range=(3, 3) -- триграммы<br/>\n",
        "ngram_range=(1, 3) -- униграммы, биграммы и триграммы."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5KabRubaXhNb"
      },
      "source": [
        "## Пример"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0EnHNZtbXlH0"
      },
      "source": [
        "К сожалению, на русском языке всё ещё очень мало годных наборов данных. Набор данных нашёл тут: https://github.com/sismetanin/rureviews"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yX-0fRF3hzCy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "405f9680-beaa-471f-f891-5f11df7d9b70"
      },
      "source": [
        "!pip install PyDrive"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: PyDrive in /usr/local/lib/python3.6/dist-packages (1.3.1)\n",
            "Requirement already satisfied: PyYAML>=3.0 in /usr/local/lib/python3.6/dist-packages (from PyDrive) (3.13)\n",
            "Requirement already satisfied: oauth2client>=4.0.0 in /usr/local/lib/python3.6/dist-packages (from PyDrive) (4.1.3)\n",
            "Requirement already satisfied: google-api-python-client>=1.2 in /usr/local/lib/python3.6/dist-packages (from PyDrive) (1.7.12)\n",
            "Requirement already satisfied: httplib2>=0.9.1 in /usr/local/lib/python3.6/dist-packages (from oauth2client>=4.0.0->PyDrive) (0.17.4)\n",
            "Requirement already satisfied: pyasn1>=0.1.7 in /usr/local/lib/python3.6/dist-packages (from oauth2client>=4.0.0->PyDrive) (0.4.8)\n",
            "Requirement already satisfied: pyasn1-modules>=0.0.5 in /usr/local/lib/python3.6/dist-packages (from oauth2client>=4.0.0->PyDrive) (0.2.8)\n",
            "Requirement already satisfied: rsa>=3.1.4 in /usr/local/lib/python3.6/dist-packages (from oauth2client>=4.0.0->PyDrive) (4.6)\n",
            "Requirement already satisfied: six>=1.6.1 in /usr/local/lib/python3.6/dist-packages (from oauth2client>=4.0.0->PyDrive) (1.15.0)\n",
            "Requirement already satisfied: uritemplate<4dev,>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from google-api-python-client>=1.2->PyDrive) (3.0.1)\n",
            "Requirement already satisfied: google-auth>=1.4.1 in /usr/local/lib/python3.6/dist-packages (from google-api-python-client>=1.2->PyDrive) (1.17.2)\n",
            "Requirement already satisfied: google-auth-httplib2>=0.0.3 in /usr/local/lib/python3.6/dist-packages (from google-api-python-client>=1.2->PyDrive) (0.0.4)\n",
            "Requirement already satisfied: setuptools>=40.3.0 in /usr/local/lib/python3.6/dist-packages (from google-auth>=1.4.1->google-api-python-client>=1.2->PyDrive) (50.3.2)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from google-auth>=1.4.1->google-api-python-client>=1.2->PyDrive) (4.1.1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aYRCckjEh2--"
      },
      "source": [
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fBBer1ozh5mL"
      },
      "source": [
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bP_tPtQDiGCx"
      },
      "source": [
        "downloaded = drive.CreateFile({'id':\"1CDqe4OJPYnsuu71IiNAPdyX5-5a2KJlI\"}) \n",
        "downloaded.GetContentFile('women-clothing-accessories.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g6709r3nXwIu"
      },
      "source": [
        "data = pd.read_csv('women-clothing-accessories.csv', sep='\\t', usecols=[0, 1])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u894TFlsYq3o"
      },
      "source": [
        "data.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "09aeAe4zvwCv"
      },
      "source": [
        "data.sample(10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DpWqBU3-ZL5F"
      },
      "source": [
        "x_train, x_test, y_train, y_test = train_test_split(data.review, data.sentiment, train_size = 0.7)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fKB-qv_obPrD"
      },
      "source": [
        "data.sentiment.value_counts()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QtowE76LbIrG"
      },
      "source": [
        "y_train.value_counts()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "muRPBnySwfih"
      },
      "source": [
        "Инициализируем `CountVectorizer()`, указав в качестве признаков униграммы:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SfavuPF9wZh_"
      },
      "source": [
        "vectorizer = CountVectorizer(ngram_range=(1, 1))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FIxErQg4wm-H"
      },
      "source": [
        "После инициализации _vectorizer_ можно обучить на наших данных. \n",
        "\n",
        "Для обучения используем обучающую выборку ```x_train```, но в отличие от классификатора мы используем метод ```fit_transform()```: сначала обучаем наш векторизатор, а потом сразу применяем его к нашему набору данных. Это похоже на то, как мы работали с one-hot-encoderом."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0iwq0gTww0Pw"
      },
      "source": [
        "vectorized_x_train = vectorizer.fit_transform(x_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Scw-UGKrwysI"
      },
      "source": [
        "Так как результат не зависит от порядка слов в текстах, то говорят, что такая модель представления текстов в виде векторов получается из *гипотезы представления текста как мешка слов*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IXZEX-AxxDp4"
      },
      "source": [
        "В `vectorizer.vocabulary_` лежит словарь, отображение слов в их индексы:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gRohdVRSxJIa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "afb23c3d-2a06-4028-c3f4-1af931219b35"
      },
      "source": [
        "list(vectorizer.vocabulary_.items())[:10]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('заказываю', 12551),\n",
              " ('второй', 7961),\n",
              " ('раз', 33393),\n",
              " ('как', 14792),\n",
              " ('всегда', 7783),\n",
              " ('все', 7781),\n",
              " ('на', 19400),\n",
              " ('высоте', 8645),\n",
              " ('носочки', 22098),\n",
              " ('подарочек', 27431)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 55
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BQCbnAk4wS_K",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f1745d96-8b40-4d8a-e0b1-1276edfbe94b"
      },
      "source": [
        "vectorized_x_train.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(62999, 44710)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 52
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CKFQVUZ11K47",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b7b81da2-da02-4b88-b2f6-4490bb82bdfa"
      },
      "source": [
        "x_train.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(62999,)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gXAcWenlxQfc"
      },
      "source": [
        "Так как теперь у нас есть **численное представление** и набор входных признаков, то мы можем обучить нашу модель"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "icUoj9EexWcs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "975ae7bb-45b2-4ff0-e0c4-c516e532f231"
      },
      "source": [
        "clf = DecisionTreeClassifier()\n",
        "clf.fit(vectorized_x_train, y_train)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',\n",
              "                       max_depth=None, max_features=None, max_leaf_nodes=None,\n",
              "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
              "                       min_samples_leaf=1, min_samples_split=2,\n",
              "                       min_weight_fraction_leaf=0.0, presort='deprecated',\n",
              "                       random_state=None, splitter='best')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 56
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hf9V0s6jI1l2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8abf42ba-46f2-4dd2-ab96-50cca8f5e0df"
      },
      "source": [
        "clf2 = MultinomialNB()\n",
        "clf2.fit(vectorized_x_train, y_train)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 57
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jM85715BxfIx"
      },
      "source": [
        "С тестовыми данными нужно проделать то же самое, что и с данными для обучения: сделать из текстов вектора, которые можно передавать в классификатор для прогноза класса объекта. \n",
        "\n",
        "У нас уже есть обученный векторизатор ```vectorizer```, поэтому используем метод ```transform()``` (просто применить его), а не ```fit_transform``` (обучить и применить)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zWX6X7UHxjkj"
      },
      "source": [
        "vectorized_x_test = vectorizer.transform(x_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SzvSUzEcxqMo"
      },
      "source": [
        "Как раньше, для получения прогноза у обученного классификатора используем метод ```predict()```.\n",
        "\n",
        "С помощью функции ```classification_report()```, которая считает сразу несколько метрик качества классификации, посмотрим на то, насколько хорошо мы предсказываем положительную или отрицательную тональность твита ."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V7gssLYYxwkK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "70abc838-ab34-41c4-dc72-6d05ebf37e22"
      },
      "source": [
        "pred = clf.predict(vectorized_x_test)\n",
        "print(classification_report(y_test, pred))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "    neautral       0.52      0.54      0.53      9021\n",
            "    negative       0.64      0.62      0.63      8988\n",
            "    positive       0.76      0.75      0.76      8992\n",
            "\n",
            "    accuracy                           0.64     27001\n",
            "   macro avg       0.64      0.64      0.64     27001\n",
            "weighted avg       0.64      0.64      0.64     27001\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PvtntoJ0JNoK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "68a3477f-ad2b-4ae0-ee73-8f82369d5605"
      },
      "source": [
        "pred2 = clf2.predict(vectorized_x_test)\n",
        "print(classification_report(y_test, pred2))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "    neautral       0.60      0.67      0.63      9021\n",
            "    negative       0.72      0.63      0.68      8988\n",
            "    positive       0.85      0.85      0.85      8992\n",
            "\n",
            "    accuracy                           0.72     27001\n",
            "   macro avg       0.72      0.72      0.72     27001\n",
            "weighted avg       0.72      0.72      0.72     27001\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "seQ_QrKmJWoF"
      },
      "source": [
        "Итак, наивный байесовский классификатор легко побил дерево решений. Дальше работаем с ним."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c6qcO2BceNsv"
      },
      "source": [
        "### Отступление: F-мера"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z_qn9NrqeW6w"
      },
      "source": [
        "Прошлый раз мы разобрали метрики качества классификации, которые выводятся из матрицы ошибок (confision matrix). \n",
        "\n",
        "**Полнота** (Sensitivity, True Positive Rate, Recall, Hit Rate) отражает какой процент объектов положительного класса мы правильно классифицировали.\n",
        "\n",
        "**Точность** (Precision, Positive Predictive Value) отражает какой процент положительных объектов (т.е. тех, что мы считаем положительными) правильно классифицирован. (Не путать с Accuracy!)\n",
        "\n",
        "Легко построить алгоритм со 100%-й полнотой: он все объекты относит к классу 1, но при этом точность может быть очень низкой. Нетрудно построить алгоритм с близкой к 100% точностью: он относит к классу 1 только те объекты, в которых уверен, при этом полнота может быть низкая.\n",
        "\n",
        "**F1-мера** (F1 score) является средним гармоническим точности и полноты, максимизация этого функционала приводит к одновременной максимизации этих двух «ортогональных критериев»\n",
        "\n",
        "$$F_1 = \\frac{2}{\\mathrm{recall}^{-1} + \\mathrm{precision}^{-1}} = 2 \\cdot \\frac{\\mathrm{precision} \\cdot \\mathrm{recall}}{\\mathrm{precision} + \\mathrm{recall}} = \\frac{\\mathrm{tp}}{\\mathrm{tp} + \\frac12 (\\mathrm{fp} + \\mathrm{fn}) } $$\n",
        "\n",
        "Также рассматривают весовое среднее гармоническое точности и полноты –  $F_\\beta$-меру:\n",
        "\n",
        "$$F_\\beta = (1 + \\beta^2) \\cdot \\frac{\\mathrm{precision} \\cdot \\mathrm{recall}}{(\\beta^2 \\cdot \\mathrm{precision}) + \\mathrm{recall}} = \\frac {(1 + \\beta^2) \\cdot \\mathrm{tp} }{(1 + \\beta^2) \\cdot \\mathrm{tp} + \\beta^2 \\cdot \\mathrm{fn} + \\mathrm{fp}}\\,$$\n",
        "\n",
        "Изменение $\\beta$ позволяет делать один из критериев (точность или полноту) важнее при оптимизации."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5yiLk1P_xYQ2"
      },
      "source": [
        "## Биграммы"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mjy5ZPmwWg-j"
      },
      "source": [
        "Попробуем сделать то же самое, используя в качестве признаков униграммы и биграммы:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JKeS-Vmv13SE"
      },
      "source": [
        "# инициализируем векторайзер \n",
        "bigram_vectorizer = CountVectorizer(ngram_range=(1, 2))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hmQHqUpRWg-k"
      },
      "source": [
        "# обучаем его и сразу применяем к x_train\n",
        "bigram_vectorized_x_train = bigram_vectorizer.fit_transform(x_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PfG5x9i91_n4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "53cb8045-37a7-4174-fd2c-28c62a39bae9"
      },
      "source": [
        "# инициализируем и обучаем классификатор\n",
        "clf = MultinomialNB()\n",
        "clf.fit(bigram_vectorized_x_train, y_train)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 63
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "twUcp7eU2E-9"
      },
      "source": [
        "# применяем обученный векторизатор к тестовым данным\n",
        "bigram_vectorized_x_test = bigram_vectorizer.transform(x_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sPsfMX7i2H1j",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9997680b-09b3-4ef6-fb2a-0c8d36110f38"
      },
      "source": [
        "# получаем предсказания и выводим информацию о качестве\n",
        "pred = clf.predict(bigram_vectorized_x_test)\n",
        "print(classification_report(y_test, pred))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "    neautral       0.62      0.67      0.64      9021\n",
            "    negative       0.72      0.67      0.69      8988\n",
            "    positive       0.88      0.86      0.87      8992\n",
            "\n",
            "    accuracy                           0.73     27001\n",
            "   macro avg       0.74      0.73      0.74     27001\n",
            "weighted avg       0.74      0.73      0.74     27001\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MonLW7AyWg-m"
      },
      "source": [
        "У меня получилось повысить точность на пару процентов по сравнению с униграммами"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cdwrMRN93D31",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "33e96bee-2cf3-4ccd-a245-97cf80817900"
      },
      "source": [
        "bigram_vectorized_x_train.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(62999, 462523)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 66
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZHzVwaMF3LPY"
      },
      "source": [
        "\"Признаков\" объектов стало на порядок больше."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D39SSh0zWg-r"
      },
      "source": [
        "## Токенизация\n",
        "\n",
        "Токенизировать - значит, поделить текст на части: слова, ключевые слова, фразы, символы и т.д., иными словами **токены**.\n",
        "\n",
        "Самый наивный способ токенизировать текст - разделить с помощью функции `split()`. Но `split` упускает очень много всего, например, не отделяет пунктуацию от слов. Кроме этого, есть ещё много менее тривиальных проблем, поэтому лучше использовать готовые токенизаторы."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hoSe08N2Wg-r"
      },
      "source": [
        "import nltk # уже знакомая нам библиотека nltk\n",
        "from nltk.tokenize import word_tokenize # готовый токенизатор библиотеки nltk"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DiDt3L8Y8god"
      },
      "source": [
        "Чтобы использовать токенизатор ```word_tokenize```, нужно сначала скачать данные для nltk о пунктуации и стоп-словах."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gPH3yMcumsdd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "85c2ab02-2e66-4d0c-80c0-6f9e2598cb80"
      },
      "source": [
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 68
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3NfDb8D_9DqD"
      },
      "source": [
        "Применим токенизацию:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zrJDGpgYWg-4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ea2b4f9c-1a03-4365-8fb6-d2014082ca1c"
      },
      "source": [
        "sentence = 'Кто же победит на выборах в США: Трамп или Байден?'\n",
        "word_tokenize(sentence)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Кто',\n",
              " 'же',\n",
              " 'победит',\n",
              " 'на',\n",
              " 'выборах',\n",
              " 'в',\n",
              " 'США',\n",
              " ':',\n",
              " 'Трамп',\n",
              " 'или',\n",
              " 'Байден',\n",
              " '?']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 69
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vxy7KGZI9bhK"
      },
      "source": [
        "Сравните с использованием ```split()```:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p52dIuSI9W6o",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "057d3f10-dc85-44c4-8b93-f0fd4016f0a3"
      },
      "source": [
        "sentence.split()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Кто',\n",
              " 'же',\n",
              " 'победит',\n",
              " 'на',\n",
              " 'выборах',\n",
              " 'в',\n",
              " 'США:',\n",
              " 'Трамп',\n",
              " 'или',\n",
              " 'Байден?']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 70
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_702Dg5OWg-5"
      },
      "source": [
        "В nltk вообще есть довольно много токенизаторов:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ps8oPYoTWg-6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e8dd95a9-0ed5-4527-db34-8787e397817a"
      },
      "source": [
        "from nltk import tokenize\n",
        "dir(tokenize)[:16]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['BlanklineTokenizer',\n",
              " 'LineTokenizer',\n",
              " 'MWETokenizer',\n",
              " 'PunktSentenceTokenizer',\n",
              " 'RegexpTokenizer',\n",
              " 'ReppTokenizer',\n",
              " 'SExprTokenizer',\n",
              " 'SpaceTokenizer',\n",
              " 'StanfordSegmenter',\n",
              " 'TabTokenizer',\n",
              " 'TextTilingTokenizer',\n",
              " 'ToktokTokenizer',\n",
              " 'TreebankWordTokenizer',\n",
              " 'TweetTokenizer',\n",
              " 'WhitespaceTokenizer',\n",
              " 'WordPunctTokenizer']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 71
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lmnGCL5iWg-8"
      },
      "source": [
        "Одни умеют выдавать индексы в строке для начала и конца каждого слова-токена:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jejj5X7QWg-8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "96cc1e39-ada7-406a-b948-3fa342d6d68e"
      },
      "source": [
        "wh_tok = tokenize.WhitespaceTokenizer()\n",
        "list(wh_tok.span_tokenize(sentence))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(0, 3),\n",
              " (4, 6),\n",
              " (7, 14),\n",
              " (15, 17),\n",
              " (18, 25),\n",
              " (26, 27),\n",
              " (28, 32),\n",
              " (33, 38),\n",
              " (39, 42),\n",
              " (43, 50)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 72
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a-wf6A1EWg--"
      },
      "source": [
        "Некторые токенизаторы ведут себя специфично:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2REwpHGWWg-_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c81701e2-05e1-429e-8da1-46bcaaba3901"
      },
      "source": [
        "tokenize.TreebankWordTokenizer().tokenize(\"don't stop me, otherwise I'll punch you\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['do', \"n't\", 'stop', 'me', ',', 'otherwise', 'I', \"'ll\", 'punch', 'you']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 76
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tckre90JWg_B"
      },
      "source": [
        "А некоторые -- вообще не для текста на естественном языке:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F1Ml3xtaWg_D",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7c855ff5-a0d7-4fe5-9cc2-273e851958c7"
      },
      "source": [
        "tokenize.SExprTokenizer().tokenize(\"(a + (b * c)) d - e * (f)\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['(a + (b * c))', 'd', '-', 'e', '*', '(f)']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 75
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bM2kvAo0_b93"
      },
      "source": [
        "**Правильный токенизатор подбирается исходя из требований задачи!**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rhVrgkSaWg_K"
      },
      "source": [
        "## Стоп-слова\n",
        "\n",
        "**Стоп-слова** - это слова, которые часто встречаются практически в любом тексте и ничего интересного не говорят о конретном документе. Для модели это просто шум. А шум нужно убирать. По аналогичной причине убирают и пунктуацию."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ld-h6WKyWg_K",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "068c1004-0f8b-455b-bbfa-9e80521ffd98"
      },
      "source": [
        "# импортируем стоп-слова из библиотеки nltk\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "# посмотрим на стоп-слова для русского языка\n",
        "print(stopwords.words('russian'))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['и', 'в', 'во', 'не', 'что', 'он', 'на', 'я', 'с', 'со', 'как', 'а', 'то', 'все', 'она', 'так', 'его', 'но', 'да', 'ты', 'к', 'у', 'же', 'вы', 'за', 'бы', 'по', 'только', 'ее', 'мне', 'было', 'вот', 'от', 'меня', 'еще', 'нет', 'о', 'из', 'ему', 'теперь', 'когда', 'даже', 'ну', 'вдруг', 'ли', 'если', 'уже', 'или', 'ни', 'быть', 'был', 'него', 'до', 'вас', 'нибудь', 'опять', 'уж', 'вам', 'ведь', 'там', 'потом', 'себя', 'ничего', 'ей', 'может', 'они', 'тут', 'где', 'есть', 'надо', 'ней', 'для', 'мы', 'тебя', 'их', 'чем', 'была', 'сам', 'чтоб', 'без', 'будто', 'чего', 'раз', 'тоже', 'себе', 'под', 'будет', 'ж', 'тогда', 'кто', 'этот', 'того', 'потому', 'этого', 'какой', 'совсем', 'ним', 'здесь', 'этом', 'один', 'почти', 'мой', 'тем', 'чтобы', 'нее', 'сейчас', 'были', 'куда', 'зачем', 'всех', 'никогда', 'можно', 'при', 'наконец', 'два', 'об', 'другой', 'хоть', 'после', 'над', 'больше', 'тот', 'через', 'эти', 'нас', 'про', 'всего', 'них', 'какая', 'много', 'разве', 'три', 'эту', 'моя', 'впрочем', 'хорошо', 'свою', 'этой', 'перед', 'иногда', 'лучше', 'чуть', 'том', 'нельзя', 'такой', 'им', 'более', 'всегда', 'конечно', 'всю', 'между']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yHg5Z93iAyee"
      },
      "source": [
        "noise = stopwords.words('russian')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t3gweXaWWg_P"
      },
      "source": [
        "Теперь нужно обучать нашу модель с учетом новых знаний про токенизацию и стоп-слова. \n",
        "\n",
        "Для этого мы можем собрать новый векторизатор, передав ему на вход:\n",
        "* какие n-граммы нам нужны, параметр **ngram_range**;\n",
        "* какой токенизатор мы используем, параметр **tokenizer**;\n",
        "* какие у нас стоп-слова, параметр **stop_words**."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fbXrVeRRuAxx"
      },
      "source": [
        "# инициализируем умный векторайзер \n",
        "smart_vectorizer = CountVectorizer(ngram_range=(1, 1), stop_words=noise)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BCM2Jy0448Mc"
      },
      "source": [
        "# обучаем его и сразу применяем к x_train\n",
        "smart_vectorized_x_train = smart_vectorizer.fit_transform(x_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UTiC4oUX5T__",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b394c47a-9a6f-44db-e651-12021602605c"
      },
      "source": [
        "smart_vectorized_x_train.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(62999, 44568)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 83
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7BztanE26o5Z",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7413f6de-9521-4df3-a992-ad36f7fed225"
      },
      "source": [
        "list(smart_vectorizer.vocabulary_.items())[:10]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('заказываю', 12511),\n",
              " ('второй', 7938),\n",
              " ('высоте', 8621),\n",
              " ('носочки', 22015),\n",
              " ('подарочек', 27337),\n",
              " ('продавец', 31778),\n",
              " ('отзывчивый', 24522),\n",
              " ('переживает', 26141),\n",
              " ('рекомендую', 34652),\n",
              " ('взяла', 6741)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 84
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7Nc6D-nwWg_P",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8640e87e-d4b9-4de1-dbae-f3de17a4b94e"
      },
      "source": [
        "# инициализируем и обучаем классификатор\n",
        "clf = MultinomialNB()\n",
        "clf.fit(smart_vectorized_x_train, y_train)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 85
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OQ-P91PV5KST"
      },
      "source": [
        "# применяем обученный векторайзер к тестовым данным\n",
        "smart_vectorized_x_test = smart_vectorizer.transform(x_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8QJ3elF85MDB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "94aa632d-e3f2-4248-bbf2-a1028b45aac0"
      },
      "source": [
        "# получаем предсказания и выводим информацию о качестве\n",
        "pred = clf.predict(smart_vectorized_x_test)\n",
        "print(classification_report(y_test, pred))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "    neautral       0.59      0.66      0.62      9021\n",
            "    negative       0.72      0.62      0.67      8988\n",
            "    positive       0.83      0.85      0.84      8992\n",
            "\n",
            "    accuracy                           0.71     27001\n",
            "   macro avg       0.71      0.71      0.71     27001\n",
            "weighted avg       0.71      0.71      0.71     27001\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DYWB1foQWg_T"
      },
      "source": [
        "Получилось чуть хуже. \n",
        "\n",
        "Что ещё можно сделать?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XsRf9T_SWg_U"
      },
      "source": [
        "## Лемматизация\n",
        "\n",
        "**Лемматизация** – это сведение разных форм одного слова к начальной форме – **лемме**. Почему это хорошо?\n",
        "* Во-первых, естественно рассматривать как отдельный признак каждое *слово*, а не каждую его отдельную форму.\n",
        "* Во-вторых, некоторые стоп-слова стоят только в начальной форме, и без лематизации выкидываем мы только её.\n",
        "\n",
        "Для русского есть хороший лемматизатор pymorphy. \n",
        "\n",
        "Стемминг (англ. stemming — находить происхождение) — это процесс нахождения основы слова для заданного исходного слова. Основа слова не обязательно совпадает с морфологическим корнем слова. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ylKZG2MwWg_f"
      },
      "source": [
        "### [Pymorphy](http://pymorphy2.readthedocs.io/en/latest/)\n",
        "Это модуль на питоне, довольно быстрый и с кучей функций."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JcYWYq4BzOon",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "05f2702b-e1df-4c42-8c0f-f5e065693714"
      },
      "source": [
        "# устанавливаем pymorphy2\n",
        "!pip install pymorphy2"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting pymorphy2\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/07/57/b2ff2fae3376d4f3c697b9886b64a54b476e1a332c67eee9f88e7f1ae8c9/pymorphy2-0.9.1-py3-none-any.whl (55kB)\n",
            "\r\u001b[K     |██████                          | 10kB 9.9MB/s eta 0:00:01\r\u001b[K     |███████████▉                    | 20kB 2.1MB/s eta 0:00:01\r\u001b[K     |█████████████████▊              | 30kB 2.7MB/s eta 0:00:01\r\u001b[K     |███████████████████████▋        | 40kB 3.0MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▌  | 51kB 2.4MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 61kB 2.0MB/s \n",
            "\u001b[?25hCollecting dawg-python>=0.7.1\n",
            "  Downloading https://files.pythonhosted.org/packages/6a/84/ff1ce2071d4c650ec85745766c0047ccc3b5036f1d03559fd46bb38b5eeb/DAWG_Python-0.7.2-py2.py3-none-any.whl\n",
            "Requirement already satisfied: docopt>=0.6 in /usr/local/lib/python3.6/dist-packages (from pymorphy2) (0.6.2)\n",
            "Collecting pymorphy2-dicts-ru<3.0,>=2.4\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/3a/79/bea0021eeb7eeefde22ef9e96badf174068a2dd20264b9a378f2be1cdd9e/pymorphy2_dicts_ru-2.4.417127.4579844-py2.py3-none-any.whl (8.2MB)\n",
            "\u001b[K     |████████████████████████████████| 8.2MB 7.8MB/s \n",
            "\u001b[?25hInstalling collected packages: dawg-python, pymorphy2-dicts-ru, pymorphy2\n",
            "Successfully installed dawg-python-0.7.2 pymorphy2-0.9.1 pymorphy2-dicts-ru-2.4.417127.4579844\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IqdT2pmRFn2F"
      },
      "source": [
        "В pymorphy2 для морфологического анализа слов есть ```MorphAnalyzer()```:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m4nRuUu2Wg_g"
      },
      "source": [
        "from pymorphy2 import MorphAnalyzer\n",
        "pymorphy2_analyzer = MorphAnalyzer()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Egd6KdqzWg_h"
      },
      "source": [
        "pymorphy2 работает с отдельными словами. Если дать ему на вход предложение - он его просто не лемматизирует, т.к. не понимает:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M6hdm1KBFx18"
      },
      "source": [
        "sentence = 'Кто же победит на выборах в США: Трамп или Байден?'\n",
        "sent = word_tokenize(sentence)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cr1C2beNF3vE"
      },
      "source": [
        "Лемматизируем слово \"победит\" из предложения ```sentence``` с помощью метода ```parse()```:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1Q3zNlPBWg_i",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5c7b48d5-932d-4f99-ff51-b57175cdcea4"
      },
      "source": [
        "ana = pymorphy2_analyzer.parse(sent[2])\n",
        "ana"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Parse(word='победит', tag=OpencorporaTag('VERB,perf,tran sing,3per,futr,indc'), normal_form='победить', score=0.846153, methods_stack=((DictionaryAnalyzer(), 'победит', 2483, 9),)),\n",
              " Parse(word='победит', tag=OpencorporaTag('NOUN,inan,masc sing,nomn'), normal_form='победит', score=0.076923, methods_stack=((DictionaryAnalyzer(), 'победит', 34, 0),)),\n",
              " Parse(word='победит', tag=OpencorporaTag('NOUN,inan,masc sing,accs'), normal_form='победит', score=0.076923, methods_stack=((DictionaryAnalyzer(), 'победит', 34, 3),))]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 91
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m2O2BL4_GJzq"
      },
      "source": [
        "Выведем его нормальную форму:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7-zp0KZLWg_p",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "6e2650a8-bcf4-43d0-e674-322b1c32c864"
      },
      "source": [
        "ana[0].normal_form"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'победить'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 92
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qUzhGLTxLIr9"
      },
      "source": [
        "Нормализация предложения \"вижу три села\" может дать \"видеть тереть сесть\""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ldFCYxXuLL4p"
      },
      "source": [
        "sent2 = word_tokenize('вижу три села')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oKRiNng-LfoP",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "ee8d1ad0-2e14-4304-ba86-0f3da77f975e"
      },
      "source": [
        "ana2 = pymorphy2_analyzer.parse(sent2[0])\n",
        "ana2[0].normal_form"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'видеть'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 96
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VlWxW3e9Wg-m"
      },
      "source": [
        "## TF-IDF векторизация"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u7hCxZRtWg-m"
      },
      "source": [
        "`TfidfVectorizer` делает то же, что и `CountVectorizer`, но в качестве значений выдает **tf-idf** каждого слова.\n",
        "\n",
        "Как считается tf-idf:\n",
        "\n",
        "**TF (term frequency)** – относительная частотность слова в документе:\n",
        "$$ TF(t,d) = \\frac{n_{t}}{\\sum_k n_{k}} $$\n",
        "\n",
        "**IDF (inverse document frequency)** – обратная частота документов, в которых есть это слово:\n",
        "$$ IDF(t, D) = \\mbox{log} \\frac{|D|}{|{d : t \\in d}|} $$\n",
        "\n",
        "Перемножаем их:\n",
        "$$TFIDF(t, d, D) = TF(t,d) \\times IDF(i, D)$$\n",
        "\n",
        "Cмысл: если слово часто встречается в одном документе, но в целом по корпусу встречается в небольшом \n",
        "количестве документов, у него высокий TF-IDF."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fv7DfTkJWg-n"
      },
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "02f_zZm14PHM"
      },
      "source": [
        "Действуем аналогично, как с ```CountVectorizer()```:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JjF9m3EOQTBK"
      },
      "source": [
        "# инициализируем векторизатор, в качестве переменных используем униграммы\n",
        "tfidf_vectorizer = TfidfVectorizer(ngram_range=(1, 1))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VrTdCw_TDE9o"
      },
      "source": [
        "# обучаем его и сразу применяем к x_train\n",
        "tfidf_vectorized_x_train = tfidf_vectorizer.fit_transform(x_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JPacf_DKDP7M",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ecf1f094-b7c4-4a17-fb78-d1dc41acf9e3"
      },
      "source": [
        "# инициализируем и обучаем классификатор\n",
        "clf = MultinomialNB()\n",
        "clf.fit(tfidf_vectorized_x_train, y_train)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 100
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ljAO8NIPDSnK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "60d0d64a-2a4e-4534-8ff2-3028f73f43a4"
      },
      "source": [
        "# применяем обученный векторизатор к тестовым данным\n",
        "tfidf_vectorized_x_test = tfidf_vectorizer.transform(x_test)\n",
        "\n",
        "# получаем предсказания и выводим информацию о качестве\n",
        "pred = clf.predict(tfidf_vectorized_x_test)\n",
        "print(classification_report(y_test, pred))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "    neautral       0.60      0.67      0.63      9021\n",
            "    negative       0.72      0.63      0.67      8988\n",
            "    positive       0.85      0.85      0.85      8992\n",
            "\n",
            "    accuracy                           0.72     27001\n",
            "   macro avg       0.72      0.72      0.72     27001\n",
            "weighted avg       0.72      0.72      0.72     27001\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9hedBdcYWhAH"
      },
      "source": [
        "Иногда пунктуация бывает и не шумом - главное отталкиваться от задачи. Что будет если вообще не убирать пунктуацию? На примере с твитами хорошо было бы видно, что пунктуация работает хорошо ))"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XhZMwsY5WhAI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "16e5e10e-2785-4c50-f77d-d916cdfbd519"
      },
      "source": [
        "# инициализируем умный векторайзер stop-words НЕ ИСПОЛЬЗУЕМ!\n",
        "alternative_tfidf_vectorizer = TfidfVectorizer(ngram_range=(1, 1), \n",
        "                                               tokenizer=word_tokenize)\n",
        "\n",
        "# обучаем его и сразу применяем к x_train\n",
        "alternative_tfidf_vectorized_x_train = alternative_tfidf_vectorizer.fit_transform(x_train)\n",
        "\n",
        "# инициализируем и обучаем классификатор\n",
        "clf = MultinomialNB()\n",
        "clf.fit(alternative_tfidf_vectorized_x_train, y_train)\n",
        "\n",
        "# применяем обученный векторайзер к тестовым данным\n",
        "alternative_tfidf_vectorized_x_test = alternative_tfidf_vectorizer.transform(x_test)\n",
        "\n",
        "# получаем предсказания и выводим информацию о качестве\n",
        "pred = clf.predict(alternative_tfidf_vectorized_x_test)\n",
        "print(classification_report(y_test, pred))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "    neautral       0.60      0.67      0.63      9021\n",
            "    negative       0.72      0.65      0.68      8988\n",
            "    positive       0.85      0.84      0.85      8992\n",
            "\n",
            "    accuracy                           0.72     27001\n",
            "   macro avg       0.73      0.72      0.72     27001\n",
            "weighted avg       0.72      0.72      0.72     27001\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FEQbCtidWhAP"
      },
      "source": [
        "Посмотрим, как один из супер-значительных токенов справится с классификацией безо всякого машинного обучения:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rhUG9qWuWhAQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d578ba2c-b105-4417-ee33-0434987761f8"
      },
      "source": [
        "cool_token = 'плохо'\n",
        "pred = ['positive' if cool_token in review else 'negative' for review in x_test]\n",
        "print(classification_report(pred, y_test))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "    neautral       0.00      0.00      0.00         0\n",
            "    negative       0.94      0.33      0.49     25767\n",
            "    positive       0.02      0.15      0.04      1234\n",
            "\n",
            "    accuracy                           0.32     27001\n",
            "   macro avg       0.32      0.16      0.18     27001\n",
            "weighted avg       0.90      0.32      0.47     27001\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JrqW55jgWhAR"
      },
      "source": [
        "## Символьные n-граммы\n",
        "\n",
        "В некоторых задачах в качестве признаков могут быть использщованы, n-граммы символов. Для этого необходимо установить в ```CountVectorizer()``` параметр ```analyzer = 'char'```, то есть анализировать символы."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o4lNhEmyWhAU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b87d00a9-7c10-4e3d-a540-a68f32600f01"
      },
      "source": [
        "# инициализируем векторайзер для символов\n",
        "char_vectorizer = CountVectorizer(analyzer='char', ngram_range=(3, 6))\n",
        "\n",
        "# обучаем его и сразу применяем к x_train\n",
        "char_vectorized_x_train = char_vectorizer.fit_transform(x_train)\n",
        "\n",
        "# инициализируем и обучаем классификатор\n",
        "clf = MultinomialNB()\n",
        "clf.fit(char_vectorized_x_train, y_train)\n",
        "\n",
        "# применяем обученный векторайзер к тестовым данным\n",
        "char_vectorized_x_test = char_vectorizer.transform(x_test)\n",
        "\n",
        "# получаем предсказания и выводим информацию о качестве\n",
        "pred = clf.predict(char_vectorized_x_test)\n",
        "print(classification_report(y_test, pred))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "    neautral       0.60      0.71      0.65      9021\n",
            "    negative       0.73      0.62      0.67      8988\n",
            "    positive       0.87      0.84      0.86      8992\n",
            "\n",
            "    accuracy                           0.72     27001\n",
            "   macro avg       0.73      0.72      0.72     27001\n",
            "weighted avg       0.73      0.72      0.72     27001\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pLMMicsFWhAY"
      },
      "source": [
        "Cимвольные n-граммы используются, например, для задачи определения языка. Ещё одна замечательная особенность признаков-символов - для них не нужна токенизация и лемматизация, можно использовать такой подход для языков, у которых нет готовых анализаторов."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bJox-LoonoPx"
      },
      "source": [
        "## Задание 5\n",
        "\n",
        "Применим полученные выше навыки и решим задачу анализа тональности отзывов. (Те, кто предпочитает работать с английским языком, могут использовать набор данных `sms_spam`, он есть в папке `Data`).\n",
        "\n",
        "Нужно повторить весь пайплайн от сырых текстов до получения обученной модели.\n",
        "\n",
        "Обязательные шаги предобработки:\n",
        "1. токенизация\n",
        "2. приведение к нижнему регистру\n",
        "3. удаление стоп-слов\n",
        "4. лемматизация\n",
        "5. векторизация (с настройкой гиперпараметров)\n",
        "6. построение модели\n",
        "7. оценка качества модели\n",
        "\n",
        "Обязательно использование векторайзеров:\n",
        "1. мешок n-грамм (диапазон для n подбирайте самостоятельно, запрещено использовать только униграммы).\n",
        "2. tf-idf ((диапазон для n подбирайте самостоятельно, также нужно подбирать параметры max_df, min_df, max_features)\n",
        "3. символьные n-граммы (диапазон для n подбирайте самостоятельно)\n",
        "\n",
        "В качестве классификатора нужно использовать наивный байесовский классификатор. \n",
        "\n",
        "Для сравнения векторайзеров между собой используйте precision, recall, f1-score и accuracy. Для этого сформируйте датафрейм, в котором в строках будут разные векторайзеры, а в столбцах разные метрики качества, а в  ячейках будут значения этих метрик для соответсвующих векторайзеров."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VZCqxPFze72i",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "89f3094a-ae95-461e-8ebd-28bf93cf2df2"
      },
      "source": [
        "!pip install pymorphy2\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import nltk \n",
        "import string\n",
        "\n",
        "from sklearn.metrics import * \n",
        "from sklearn.model_selection import train_test_split \n",
        "from sklearn.naive_bayes import MultinomialNB \n",
        "from sklearn.feature_extraction.text import CountVectorizer \n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "\n",
        "from pymorphy2 import MorphAnalyzer\n",
        "\n",
        "from tqdm import tqdm\n",
        "\n",
        "\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pymorphy2 in /usr/local/lib/python3.6/dist-packages (0.9.1)\n",
            "Requirement already satisfied: dawg-python>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from pymorphy2) (0.7.2)\n",
            "Requirement already satisfied: docopt>=0.6 in /usr/local/lib/python3.6/dist-packages (from pymorphy2) (0.6.2)\n",
            "Requirement already satisfied: pymorphy2-dicts-ru<3.0,>=2.4 in /usr/local/lib/python3.6/dist-packages (from pymorphy2) (2.4.417127.4579844)\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7ConzWa9bgrs"
      },
      "source": [
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e7DDBqBjbnJU"
      },
      "source": [
        "downloaded = drive.CreateFile({'id':\"1CDqe4OJPYnsuu71IiNAPdyX5-5a2KJlI\"}) \n",
        "downloaded.GetContentFile('women-clothing-accessories.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tY6Zk-sTbvvV"
      },
      "source": [
        "data1 = pd.read_csv('women-clothing-accessories.csv', sep='\\t', usecols=[0, 1])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4AMVfEVxde32",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 195
        },
        "outputId": "dd7bf1b4-a949-4a0b-d022-a3b183292ad8"
      },
      "source": [
        "data1.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>review</th>\n",
              "      <th>sentiment</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>качество плохое пошив ужасный (горловина напер...</td>\n",
              "      <td>negative</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Товар отдали другому человеку, я не получила п...</td>\n",
              "      <td>negative</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Ужасная синтетика! Тонкая, ничего общего с пре...</td>\n",
              "      <td>negative</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>товар не пришел, продавец продлил защиту без м...</td>\n",
              "      <td>negative</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Кофточка голая синтетика, носить не возможно.</td>\n",
              "      <td>negative</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                              review sentiment\n",
              "0  качество плохое пошив ужасный (горловина напер...  negative\n",
              "1  Товар отдали другому человеку, я не получила п...  negative\n",
              "2  Ужасная синтетика! Тонкая, ничего общего с пре...  negative\n",
              "3  товар не пришел, продавец продлил защиту без м...  negative\n",
              "4      Кофточка голая синтетика, носить не возможно.  negative"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tEQuxAqLeVNl",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 343
        },
        "outputId": "bf9266e0-f985-4474-f8a5-10259e96f944"
      },
      "source": [
        "data1.sample(10)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>review</th>\n",
              "      <th>sentiment</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>11821</th>\n",
              "      <td>ужасно, качество материала просто фу, написано...</td>\n",
              "      <td>negative</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6230</th>\n",
              "      <td>после открытия спора пришла смс, что деньги во...</td>\n",
              "      <td>negative</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>47380</th>\n",
              "      <td>Заказ шел не очень долго,блуза в целом неплоха...</td>\n",
              "      <td>neautral</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>30160</th>\n",
              "      <td>швы не ровные и качество материала не очень. з...</td>\n",
              "      <td>neautral</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7512</th>\n",
              "      <td>Размер s очень большой, сам пошив не понятный ...</td>\n",
              "      <td>negative</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23021</th>\n",
              "      <td>на лифе затяжка,с внутренней стороны строчка р...</td>\n",
              "      <td>negative</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>49866</th>\n",
              "      <td>Футболка синтетика и жутко ваняет</td>\n",
              "      <td>neautral</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>87908</th>\n",
              "      <td>влюблена в это мягчайший свитер! Хорошо тянетс...</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>52845</th>\n",
              "      <td>Нормальная рубашка, пойдет</td>\n",
              "      <td>neautral</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>43097</th>\n",
              "      <td>я ничего не понимаю! вскрыла пакет, который бы...</td>\n",
              "      <td>neautral</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                  review sentiment\n",
              "11821  ужасно, качество материала просто фу, написано...  negative\n",
              "6230   после открытия спора пришла смс, что деньги во...  negative\n",
              "47380  Заказ шел не очень долго,блуза в целом неплоха...  neautral\n",
              "30160  швы не ровные и качество материала не очень. з...  neautral\n",
              "7512   Размер s очень большой, сам пошив не понятный ...  negative\n",
              "23021  на лифе затяжка,с внутренней стороны строчка р...  negative\n",
              "49866                 Футболка синтетика и жутко ваняет   neautral\n",
              "87908  влюблена в это мягчайший свитер! Хорошо тянетс...  positive\n",
              "52845                         Нормальная рубашка, пойдет  neautral\n",
              "43097  я ничего не понимаю! вскрыла пакет, который бы...  neautral"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TP2lfrEKoTZo"
      },
      "source": [
        "def lemmatize(x_data):\n",
        "  pymrth_analyzer = MorphAnalyzer()\n",
        "  sentences = []\n",
        "  \n",
        "  for j in range(len(x_data)):\n",
        "    sentence = x_data[j]\n",
        "\n",
        "    for ch in string.punctuation:\n",
        "      sentence = sentence.replace(ch,\"\")\n",
        "\n",
        "    sent = word_tokenize(sentence)\n",
        "    normal_sent = \"\"\n",
        "    \n",
        "    for i in range(len(sent)):\n",
        "      word = pymrth_analyzer.parse(sent[i])\n",
        "      normal_word = word[0].normal_form\n",
        "      normal_sent += (normal_word + \" \") \n",
        "\n",
        "    sentences.append(normal_sent)\n",
        "\n",
        "  return pd.Series(sentences)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "74UgW5yLdGpn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "accd5d6f-ffb5-4361-934a-af3e01d436f5"
      },
      "source": [
        "lemmatized_data_review = lemmatize(data1.review)\n",
        "lemmatized_data_review"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0        качество плохой пошив ужасный горловина напере...\n",
              "1        товар отдать другой человек я не получить посы...\n",
              "2        ужасный синтетик тонкий ничего общий с предста...\n",
              "3        товар не прийти продавец продлить защита без м...\n",
              "4              кофточка голый синтетик носить не возможно \n",
              "                               ...                        \n",
              "89995    сделать достаточно хорошо на ткань сделать рис...\n",
              "89996    накидка шикарный спасибо большой провдо линять...\n",
              "89997    спасибо большой продовца рекомендовать заказат...\n",
              "89998    очень довольный заказ маленький месяц в рб кур...\n",
              "89999    хороший куртка посторонний запах нет шов ровны...\n",
              "Length: 90000, dtype: object"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eNdRbr0bwSDA",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "7cf7449b-7fc8-4687-a6d4-c3dc4300d68b"
      },
      "source": [
        "lemmatized_data_review[0]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'качество плохой пошив ужасный горловина наперекос фото не соответствовать ткань ужасный рисунок блёклый маленький рукав не такой ужас не стоить за такой деньга г '"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QIKUmKbne1oB"
      },
      "source": [
        "x_train, x_test, y_train, y_test = train_test_split(lemmatized_data_review, data1.sentiment, train_size = 0.7)\n",
        "noise = stopwords.words('russian')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CsanMSWje-J-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "04ad860a-3254-4b25-a862-e31ac416fc34"
      },
      "source": [
        "# n-gram bag\n",
        "n_bag_vectorizer = CountVectorizer(ngram_range=(1,7), stop_words=noise, tokenizer=word_tokenize)\n",
        "n_bag_vectorizer_x_train = n_bag_vectorizer.fit_transform(x_train)\n",
        "\n",
        "clf = MultinomialNB()\n",
        "clf.fit(n_bag_vectorizer_x_train, y_train)\n",
        "\n",
        "n_bag_vectorizer_x_test = n_bag_vectorizer.transform(x_test)\n",
        "\n",
        "pred = clf.predict(n_bag_vectorizer_x_test)\n",
        "print(classification_report(y_test, pred))\n",
        "\n",
        "bag_accuracy = accuracy_score(y_test, pred)\n",
        "bag_recall = recall_score(y_test, pred, average='weighted')\n",
        "bag_precision = precision_score(y_test, pred, average='weighted')\n",
        "bag_f1 = f1_score(y_test, pred, average='weighted')\n",
        "\n",
        "bag_metrics = [bag_accuracy, bag_recall, bag_precision, bag_f1]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "    neautral       0.61      0.61      0.61      9067\n",
            "    negative       0.71      0.68      0.70      8987\n",
            "    positive       0.81      0.84      0.83      8947\n",
            "\n",
            "    accuracy                           0.71     27001\n",
            "   macro avg       0.71      0.71      0.71     27001\n",
            "weighted avg       0.71      0.71      0.71     27001\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vceC7CA18ppN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c226fec0-308d-4fe0-f0b8-b900da11e98c"
      },
      "source": [
        "# tf-idf \n",
        "tf_idf_vectorizer = TfidfVectorizer(ngram_range=(2,2), max_df=30000, min_df=1, max_features=110000)\n",
        "tf_idf_vectorizer_x_train = tf_idf_vectorizer.fit_transform(x_train)\n",
        "\n",
        "clf = MultinomialNB()\n",
        "clf.fit(tf_idf_vectorizer_x_train, y_train)\n",
        "\n",
        "tf_idf_vectorizer_x_test = tf_idf_vectorizer.transform(x_test)\n",
        "\n",
        "pred = clf.predict(tf_idf_vectorizer_x_test)\n",
        "print(classification_report(y_test, pred))\n",
        "\n",
        "tf_idf_accuracy = accuracy_score(y_test, pred)\n",
        "tf_idf_recall = recall_score(y_test, pred, average='weighted')\n",
        "tf_idf_precision = precision_score(y_test, pred, average='weighted')\n",
        "tf_idf_f1 = f1_score(y_test, pred, average='weighted')\n",
        "\n",
        "tf_idf_metrics = [tf_idf_accuracy, tf_idf_recall, tf_idf_precision, tf_idf_f1]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "    neautral       0.61      0.64      0.62      9067\n",
            "    negative       0.72      0.66      0.69      8987\n",
            "    positive       0.81      0.85      0.83      8947\n",
            "\n",
            "    accuracy                           0.72     27001\n",
            "   macro avg       0.72      0.72      0.72     27001\n",
            "weighted avg       0.72      0.72      0.72     27001\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EgGNXCmL-g0A",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "23a8cc2b-0d4e-4029-d0ad-f6cbe696d3bf"
      },
      "source": [
        "# символьные n-граммы\n",
        "char_vectorizer = CountVectorizer(analyzer='char', ngram_range=(2, 5))\n",
        "char_vectorized_x_train = char_vectorizer.fit_transform(x_train)\n",
        "\n",
        "clf = MultinomialNB()\n",
        "clf.fit(char_vectorized_x_train, y_train)\n",
        "\n",
        "char_vectorized_x_test = char_vectorizer.transform(x_test)\n",
        "\n",
        "pred = clf.predict(char_vectorized_x_test)\n",
        "print(classification_report(y_test, pred))\n",
        "\n",
        "char_bag_accuracy = accuracy_score(y_test, pred)\n",
        "char_bag_recall = recall_score(y_test, pred, average='weighted')\n",
        "char_bag_precision = precision_score(y_test, pred, average='weighted')\n",
        "char_bag_f1 = f1_score(y_test, pred, average='weighted')\n",
        "\n",
        "char_bag_metrics = [char_bag_accuracy, char_bag_recall, char_bag_precision, char_bag_f1]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "    neautral       0.58      0.69      0.63      9067\n",
            "    negative       0.73      0.61      0.67      8987\n",
            "    positive       0.85      0.81      0.83      8947\n",
            "\n",
            "    accuracy                           0.71     27001\n",
            "   macro avg       0.72      0.71      0.71     27001\n",
            "weighted avg       0.72      0.71      0.71     27001\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qo6C2l78EuXw",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        },
        "outputId": "483a89b1-f611-47e9-ef31-fa72adb42b07"
      },
      "source": [
        "all_vectorizers_metrics = [bag_metrics, tf_idf_metrics, char_bag_metrics]\n",
        "\n",
        "metrics_df = pd.DataFrame(all_vectorizers_metrics, columns=['accuracy', 'recall', 'precision', 'f1-score'], index=['n-gram bag', 'tf-idf', 'char n-gram'])\n",
        "metrics_df"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>accuracy</th>\n",
              "      <th>recall</th>\n",
              "      <th>precision</th>\n",
              "      <th>f1-score</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>n-gram bag</th>\n",
              "      <td>0.710900</td>\n",
              "      <td>0.710900</td>\n",
              "      <td>0.709064</td>\n",
              "      <td>0.709752</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>tf-idf</th>\n",
              "      <td>0.716048</td>\n",
              "      <td>0.716048</td>\n",
              "      <td>0.715965</td>\n",
              "      <td>0.715271</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>char n-gram</th>\n",
              "      <td>0.707455</td>\n",
              "      <td>0.707455</td>\n",
              "      <td>0.718756</td>\n",
              "      <td>0.709655</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "             accuracy    recall  precision  f1-score\n",
              "n-gram bag   0.710900  0.710900   0.709064  0.709752\n",
              "tf-idf       0.716048  0.716048   0.715965  0.715271\n",
              "char n-gram  0.707455  0.707455   0.718756  0.709655"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5QYTwyMtWhAZ"
      },
      "source": [
        "## Бонус 1. Регулярные выражения\n",
        "\n",
        "Регулярные выражения - способ поиска и анализа строк. Например, можно понять, какие даты в наборе строк представлены в формате DD/MM/YYYY, а какие - в других форматах. \n",
        "\n",
        "Или бывает, например, что перед работой с текстом, надо почистить его от своеобразного мусора: упоминаний пользователей, url и так далее.\n",
        "\n",
        "Навык полезный, давайте в нём тоже потренируемся.\n",
        "\n",
        "Для работы с регулярными выражениями есть библиотека **re**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VaUW5S4gWhAb"
      },
      "source": [
        "import re"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D6aYh7Osl8xr"
      },
      "source": [
        "В регулярных выражениях, кроме привычных символов-букв, есть специальные символы:\n",
        "* **?а** - ноль или один символ **а**\n",
        "* **+а** - один или более символов **а**\n",
        "* **\\*а** - ноль или более символов **а** (не путать с +)\n",
        "* **.** - любое количество любого символа\n",
        "\n",
        "Пример:\n",
        "Выражению \\*a?b. соответствуют последовательности a, ab, abc, aa, aac НО НЕ abb!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q7zOFFA3l_KQ"
      },
      "source": [
        "Рассмотрим подробно несколько наиболее полезных функций:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DbJrUpARWhAd"
      },
      "source": [
        "### findall\n",
        "возвращает список всех найденных непересекающихся совпадений.\n",
        "\n",
        "Регулярное выражение **ab+c.**: \n",
        "* **a** - просто символ **a**\n",
        "* **b+** - один или более символов **b**\n",
        "* **c** - просто символ **c**\n",
        "* **.** - любой символ\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2athHzKuWhAd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ff2d7b25-31bb-4877-bd50-95442e6a8158"
      },
      "source": [
        "result = re.findall('ab+c.', 'abcdefghijkabcabcxabc') \n",
        "print(result)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['abcd', 'abca']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A9FpIw5RWhAf"
      },
      "source": [
        "Вопрос на внимательность: почему нет abcx?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B5ttzoxEWhAg"
      },
      "source": [
        "**Задание**: вернуть список первых двух букв каждого слова в строке, состоящей из нескольких слов."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7ZR2AEq3WhAg"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MI18l-l9WhAk"
      },
      "source": [
        "### split\n",
        "разделяет строку по заданному шаблону\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sVKdRoc1WhAl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "78b5d289-8e8c-4621-fe3c-34aa130c727c"
      },
      "source": [
        "result = re.split(',', 'itsy, bitsy, teenie, weenie') \n",
        "print(result)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['itsy', ' bitsy', ' teenie', ' weenie']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "10u5efuSWhAm"
      },
      "source": [
        "можно указать максимальное количество разбиений"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9U9EQZMwWhAn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "45699604-9950-4185-cd4f-0e3ad6655629"
      },
      "source": [
        "result = re.split(',', 'itsy, bitsy, teenie, weenie', maxsplit=2) \n",
        "print(result)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['itsy', ' bitsy', ' teenie, weenie']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0EMcMyflWhAp"
      },
      "source": [
        "**Задание**: разбейте строку, состоящую из нескольких предложений, по точкам, но не более чем на 3 предложения."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dVgPSjEOWhAp"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1wrEGqBSWhAr"
      },
      "source": [
        "### sub\n",
        "ищет шаблон в строке и заменяет все совпадения на указанную подстроку\n",
        "\n",
        "параметры: (pattern, repl, string)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "az3KxKWwWhAr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "93f260be-ce79-405c-b21a-5ca8345ff4b7"
      },
      "source": [
        "result = re.sub('a', 'b', 'abcabc')\n",
        "print (result)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "bbcbbc\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qD0n7_HPWhAt"
      },
      "source": [
        "**Задание**: напишите регулярное выражение, которое позволит заменить все цифры в строке на \"DIG\"."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s_Sdu7xlWhAu"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b8__oi1PWhAv"
      },
      "source": [
        "**Задание**: напишите  регулярное выражение, которое позволит убрать url из строки."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KwNS9zt4WhAv"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gStgBJy2WhAx"
      },
      "source": [
        "### compile\n",
        "компилирует регулярное выражение в отдельный объект"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JstTupisWhAy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f60e13a6-9d93-46e5-f329-620f6072715f"
      },
      "source": [
        "# Пример: построение списка всех слов строки:\n",
        "prog = re.compile('[А-Яа-яё\\-]+')\n",
        "prog.findall(\"Слова? Да, больше, ещё больше слов! Что-то ещё.\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Слова', 'Да', 'больше', 'ещё', 'больше', 'слов', 'Что-то', 'ещё']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 147
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WXEXc3G0WhA2"
      },
      "source": [
        "**Задание**: для выбранной строки постройте список слов, которые длиннее трех символов."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nFvnIWbUWhA2"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SQDNZ3HQWhA3"
      },
      "source": [
        "**Задание**: вернуть список доменов (@gmail.com) из списка адресов электронной почты:\n",
        "\n",
        "```\n",
        "abc.test@gmail.com, xyz@test.in, test.first@analyticsvidhya.com, first.test@rest.biz\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "haZ5qn3DWhA3"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BPgOkJXhZxJ-"
      },
      "source": [
        "## Бонус 2: Word2Vec\n",
        "\n",
        "Векторные модели, которые мы рассматривали до этого (BOW, мешок слов; TF-IDF), условно называются *счётными*. Они основываются на том, что так или иначе \"считают\" слова и их соседей, и на основе этого строят вектора для слов. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CL7VqGHwZmUG"
      },
      "source": [
        "\n",
        "Другой класс моделей, который повсеместно распространён на сегодняшний день, называется *предсказательными* моделями. Идея этих моделей заключается в использовании нейросетевых архитектур, которые \"предсказывают\" (а не считают) соседей для каждого слова.\n",
        "\n",
        "Одной из самых известных таких моделей является `word2vec`. Технология основана на нейронной сети, предсказывающей вероятность встретить слово в заданном контексте. Этот инструмент был разработан группой исследователей Google в 2013 году, руководителем проекта был Томаш Миколов (сейчас работает в Facebook). Вот две самые главные статьи:\n",
        "\n",
        "* [Efficient Estimation of Word Representations in Vector Space](https://arxiv.org/pdf/1301.3781.pdf)\n",
        "* [Distributed Representations of Words and Phrases and their Compositionality](https://arxiv.org/abs/1310.4546)\n",
        "\n",
        "\n",
        "Полученные таким образом вектора называются *распределенными представлениями слов*, вложениями или **эмбеддингами**."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6V7k_qL1Z-UG"
      },
      "source": [
        "### Как это обучается?\n",
        "Мы задаём вектор для каждого слова с помощью матрицы $w$ и вектор контекста с помощью матрицы $W$. По сути, word2vec является обобщающим названием для двух архитектур Skip-Gram и Continuous Bag-Of-Words (CBOW).  \n",
        "\n",
        "![](https://www.researchgate.net/profile/Daniel_Braun6/publication/326588219/figure/fig1/AS:652185784295425@1532504616288/Continuous-Bag-of-words-CBOW-CB-and-Skip-gram-SG-training-model-illustrations.png)\n",
        "\n",
        "**CBOW** предсказывает текущее слово, исходя из окружающего его контекста. \n",
        "\n",
        "**Skip-gram**, наоборот, использует текущее слово, чтобы предугадывать окружающие его слова. \n",
        "\n",
        "### Как это работает?\n",
        "Word2vec принимает большой текстовый корпус в качестве входных данных и сопоставляет каждому слову вектор, выдавая координаты слов на выходе. Сначала он создает словарь, «обучаясь» на входных текстовых данных, а затем вычисляет векторное представление слов. \n",
        "\n",
        "Векторное представление основывается на *контекстной близости*: слова, встречающиеся в тексте рядом с одинаковыми словами (а следовательно, согласно дистрибутивной гипотезе, имеющие схожий смысл), в векторном представлении будут иметь близкие координаты векторов-слов. Для вычисления близости слов используется косинусное расстояние между их векторами.\n",
        "\n",
        "\n",
        "С помощью дистрибутивных векторных моделей можно строить семантические пропорции (они же аналогии: А относится к B так же, как C относится к D) и решать примеры:\n",
        "\n",
        "* *король: мужчина = королева: женщина* \n",
        " $\\Rightarrow$ \n",
        "* *король - мужчина + женщина = королева*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l0MPc-3NaOoE"
      },
      "source": [
        "![w2v](https://cdn-images-1.medium.com/max/2600/1*sXNXYfAqfLUeiDXPCo130w.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "29tswg7NalHk"
      },
      "source": [
        "Для слов нет лейблов, но мы используем слова в контексте для сбора обучающей выборки.\n",
        "\n",
        "## Skip gram\n",
        "\n",
        "(Предсказание контекста по слову, один из основных параметров - windows_size)\n",
        "\n",
        "![Замещающий текст](http://mccormickml.com/assets/word2vec/training_data.png)\n",
        "\n",
        "1. Представляем корпус текста в формате One-hot encoding, подаем вектор на вход нейросети\n",
        "\n",
        "2. В качестве активации последнего слоя используем softmax -> переходим в пространство вероятностей (как будто задача классификации с очень большим количеством классов)\n",
        "\n",
        "3. Предсказываем слово контекста по максимальной вероятности\n",
        "\n",
        "![Замещающий текст](https://miro.medium.com/max/875/0*FD_ZSVKFywSg-CJM.png)\n",
        "\n",
        "Модель хорошо работает с небольшим количеством тренировочных данных\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tTVRguH2Ryef"
      },
      "source": [
        "## CBOW\n",
        "\n",
        "![](https://iksinc.files.wordpress.com/2015/04/screen-shot-2015-04-12-at-10-58-21-pm.png)\n",
        "\n",
        "Тренируется быстрее, чем SkipGram, лучше точность на редких словах\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r7KSbkvwa76d"
      },
      "source": [
        "### Проблемы\n",
        "Невозможно установить тип семантических отношений между словами: синонимы, антонимы и т.д. будут одинаково близки, потому что обычно употребляются в схожих контекстах. Поэтому близкие в векторном пространстве слова называют *семантическими ассоциатами*. Это значит, что они семантически связаны, но как именно — непонятно.\n",
        "\n",
        "\n",
        "## RusVectōrēs\n",
        "\n",
        "\n",
        "На сайте [RusVectōrēs](https://rusvectores.org/ru/) собраны предобученные на различных данных модели для русского языка, а также можно поискать наиболее близкие слова к заданному, посчитать семантическую близость нескольких слов и порешать примеры с помощью «калькулятора семантической близости».\n",
        "\n",
        "\n",
        "Для других языков также можно найти предобученные модели — например, модели [fastText](https://fasttext.cc/docs/en/english-vectors.html) и [GloVe](https://nlp.stanford.edu/projects/glove/)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SwcFQkV7bA0M"
      },
      "source": [
        "## Gensim\n",
        "\n",
        "Использовать предобученную модель эмбеддингов или обучить свою можно с помощью библиотеки `gensim`. Вот [ее документация](https://radimrehurek.com/gensim/models/word2vec.html).\n",
        "\n",
        "### Как использовать готовую модель\n",
        "\n",
        "Модели word2vec бывают разных форматов:\n",
        "\n",
        "* .vec.gz — обычный файл (текстовый)\n",
        "* .bin.gz — бинарный файл\n",
        "\n",
        "Загружаются они с помощью одного и того же класса `KeyedVectors`, меняется только параметр `binary` у функции `load_word2vec_format`. \n",
        "\n",
        "Если же эмбеддинги обучены **не** с помощью word2vec, то для загрузки нужно использовать функцию `load`. Т.е. **для загрузки предобученных эмбеддингов *glove, fasttext, bpe* и любых других нужна именно она**.\n",
        "\n",
        "Скачаем с RusVectōrēs модель для русского языка, обученную на НКРЯ образца 2015 г. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kTnxWXhubJ3W"
      },
      "source": [
        "import urllib.request # библиотека для скачивания данных\n",
        "import gensim # библиотека для загрузки и использвоания моделй w2v\n",
        "from gensim.models import word2vec # непосредственно методы w2v"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yhY2nOyTbM1h",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e60b0622-484b-47b3-a974-5e4adebf3ded"
      },
      "source": [
        "# скачиваем модель ruscorpora_mystem_cbow_300 с сайта rusvectores\n",
        "# 300 - размерность вектора embeddings для слов\n",
        "\n",
        "urllib.request.urlretrieve(\"http://rusvectores.org/static/models/rusvectores2/ruscorpora_mystem_cbow_300_2_2015.bin.gz\", \"ruscorpora_mystem_cbow_300_2_2015.bin.gz\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('ruscorpora_mystem_cbow_300_2_2015.bin.gz',\n",
              " <http.client.HTTPMessage at 0x7f6c55f46438>)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 75
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iIY8HdoEbRCE"
      },
      "source": [
        "Загружаем скачанную модель. Обратите внимание, что мы скачали бинарный файл (.bin.gz), поэтому у функции load_word2vec_format() параметр binary=True"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w0YvC5bBbVVm"
      },
      "source": [
        "model_path = 'ruscorpora_mystem_cbow_300_2_2015.bin.gz'\n",
        "\n",
        "model_ru = gensim.models.KeyedVectors.load_word2vec_format(model_path, binary=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "udp_Onx4bX3D"
      },
      "source": [
        "Посмотрим на ближайших соседей следующей группы слов:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y7WoLtzabZ57"
      },
      "source": [
        "words = ['день_S', 'ночь_S', 'человек_S', 'семантика_S', 'биткоин_S']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W1wix4nwbfLf"
      },
      "source": [
        "Частеречные тэги (например, _S, тег части речи слова) нужны, поскольку это специфика скачанной модели - она была натренирована на словах, размеченных по частям речи (и лемматизированных). \n",
        "\n",
        "**NB!** В названиях моделей на `rusvectores` указано, какой тегсет (набор обозначений тегов) они используют (mystem, upos и т.д.)\n",
        "\n",
        "Попросим у модели 10 ближайших соседей для каждого слова и косинусные близости для каждого:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_xIsRf1Ebhrd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ca1ef666-ac6e-471d-938b-685727e30e3d"
      },
      "source": [
        "for word in words:\n",
        "    # есть ли слово в модели? \n",
        "    if word in model_ru:\n",
        "        print(word)\n",
        "        # смотрим на вектор слова (его размерность 300, смотрим на первые 10 чисел)\n",
        "        print(model_ru[word][:10])\n",
        "        # выдаем 10 ближайших соседей слова:\n",
        "        for word, sim in model_ru.most_similar(positive=[word], topn=10):\n",
        "            # слово + коэффициент косинусной близости\n",
        "            print(word, ': ', sim)\n",
        "        print('\\n')\n",
        "    else:\n",
        "        # Увы!\n",
        "        print('Увы, слова \"%s\" нет в модели!' % word)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "день_S\n",
            "[-0.02580778  0.00970898  0.01941961 -0.02332282  0.02017624  0.07275085\n",
            " -0.01444375  0.03316632  0.01242602  0.02833412]\n",
            "неделя_S :  0.7165195941925049\n",
            "месяц_S :  0.631048858165741\n",
            "вечер_S :  0.5828739404678345\n",
            "утро_S :  0.5676207542419434\n",
            "час_S :  0.5605547428131104\n",
            "минута_S :  0.5297019481658936\n",
            "гекатомбеон_S :  0.4897990822792053\n",
            "денек_S :  0.48224714398384094\n",
            "полчаса_S :  0.48217129707336426\n",
            "ночь_S :  0.478074848651886\n",
            "\n",
            "\n",
            "ночь_S\n",
            "[-0.00688948  0.00408364  0.06975466 -0.00959525  0.0194835   0.04057068\n",
            " -0.00994112  0.06064967 -0.00522624  0.00520327]\n",
            "вечер_S :  0.6946247816085815\n",
            "утро_S :  0.57301926612854\n",
            "ноченька_S :  0.5582467317581177\n",
            "рассвет_S :  0.5553582906723022\n",
            "ночка_S :  0.5351512432098389\n",
            "полдень_S :  0.5334426164627075\n",
            "полночь_S :  0.478694349527359\n",
            "день_S :  0.4780748784542084\n",
            "сумерки_S :  0.4390218257904053\n",
            "фундерфун_S :  0.4340824782848358\n",
            "\n",
            "\n",
            "человек_S\n",
            "[ 0.02013756 -0.02670703 -0.02039861 -0.05477146  0.00086402 -0.01636335\n",
            "  0.04240306 -0.00025525 -0.14045681  0.04785006]\n",
            "женщина_S :  0.5979775190353394\n",
            "парень_S :  0.4991787374019623\n",
            "мужчина_S :  0.4767409563064575\n",
            "мужик_S :  0.47384002804756165\n",
            "россиянин_S :  0.47190436720848083\n",
            "народ_S :  0.4654741883277893\n",
            "согражданин_S :  0.45378512144088745\n",
            "горожанин_S :  0.44368088245391846\n",
            "девушка_S :  0.44314485788345337\n",
            "иностранец_S :  0.43849867582321167\n",
            "\n",
            "\n",
            "семантика_S\n",
            "[-0.03066749  0.0053851   0.1110732   0.0152335   0.00440643  0.00384104\n",
            "  0.00096944 -0.03538784 -0.00079585  0.03220548]\n",
            "семантический_A :  0.5334584712982178\n",
            "понятие_S :  0.5030269622802734\n",
            "сочетаемость_S :  0.4817051291465759\n",
            "актант_S :  0.47596412897109985\n",
            "хронотоп_S :  0.46330299973487854\n",
            "метафора_S :  0.46158894896507263\n",
            "мышление_S :  0.4610119163990021\n",
            "парадигма_S :  0.45796656608581543\n",
            "лексема_S :  0.45688074827194214\n",
            "смысловой_A :  0.4543077349662781\n",
            "\n",
            "\n",
            "Увы, слова \"биткоин_S\" нет в модели!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/gensim/matutils.py:737: FutureWarning: Conversion of the second argument of issubdtype from `int` to `np.signedinteger` is deprecated. In future, it will be treated as `np.int64 == np.dtype(int).type`.\n",
            "  if np.issubdtype(vec.dtype, np.int):\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oSKvti4ObmKP"
      },
      "source": [
        "Найдем похожесть пары слов функцией ```similarity()``` (там используется косинусная мера схожести):"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0dbBhOf3bvZj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8f8d44de-b63d-4466-e2c5-d1e6ad9612ca"
      },
      "source": [
        "print(model_ru.similarity('человек_S', 'обезьяна_S'))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.23895611\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/gensim/matutils.py:737: FutureWarning: Conversion of the second argument of issubdtype from `int` to `np.signedinteger` is deprecated. In future, it will be treated as `np.int64 == np.dtype(int).type`.\n",
            "  if np.issubdtype(vec.dtype, np.int):\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y2vjNfsXb1bF"
      },
      "source": [
        "У загруженной модели много различных функций. Например, можно решать задачи на семантическую близость.\n",
        "\n",
        "Что получится, если вычесть из пиццы Италию и прибавить Сибирь?\n",
        "\n",
        "Для решения примера в качестве параметров метода ```most_similar()``` необходимо передать:\n",
        "* positive — вектора, которые мы складываем\n",
        "* negative — вектора, которые вычитаем\n",
        "\n",
        "*Замечание:* не забываем взять самый близкий элемент, для этого необходимо указать ```[0][0]```."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0HE75DAGb40E"
      },
      "source": [
        "Что получится, если вычесть из пиццы Италию и прибавить Сибирь?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nz81U7aKb8KD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "de0c9376-1bbe-4976-ff9f-c25209745fdf"
      },
      "source": [
        "print(model_ru.most_similar(negative=[ 'италия_S'], positive=['пицца_S','сибирь_S'])[0][0])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "пельмень_S\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/gensim/matutils.py:737: FutureWarning: Conversion of the second argument of issubdtype from `int` to `np.signedinteger` is deprecated. In future, it will be treated as `np.int64 == np.dtype(int).type`.\n",
            "  if np.issubdtype(vec.dtype, np.int):\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yWRxadscb-oY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0e25443a-54cf-4d9b-f60e-c4edbb699162"
      },
      "source": [
        "print(model_ru.most_similar(positive=['футбол_S', 'хоккей_S'], negative=['россия_S'])[0][0])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "волейбол_S\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/gensim/matutils.py:737: FutureWarning: Conversion of the second argument of issubdtype from `int` to `np.signedinteger` is deprecated. In future, it will be treated as `np.int64 == np.dtype(int).type`.\n",
            "  if np.issubdtype(vec.dtype, np.int):\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mfmyC1sQcAme"
      },
      "source": [
        "**Задание.** Придумайте и проверьте с помощью метода `most_similar` несколько аналогий"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "apY4f9DHcIn7"
      },
      "source": [
        "Метод ```doesnt_match()``` находит \"лишнее слово\" в группе слов:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LoHVj3XscPTf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 126
        },
        "outputId": "dca2f14e-c856-4e61-aece-6496ce609af8"
      },
      "source": [
        "model_ru.doesnt_match('пицца_S пельмень_S хот-дог_S ананас_S'.split())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/gensim/models/keyedvectors.py:895: FutureWarning: arrays to stack must be passed as a \"sequence\" type such as list or tuple. Support for non-sequence iterables such as generators is deprecated as of NumPy 1.16 and will raise an error in the future.\n",
            "  vectors = vstack(self.word_vec(word, use_norm=True) for word in used_words).astype(REAL)\n",
            "/usr/local/lib/python3.6/dist-packages/gensim/matutils.py:737: FutureWarning: Conversion of the second argument of issubdtype from `int` to `np.signedinteger` is deprecated. In future, it will be treated as `np.int64 == np.dtype(int).type`.\n",
            "  if np.issubdtype(vec.dtype, np.int):\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'ананас_S'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 82
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rz5cyw-ScRFd"
      },
      "source": [
        "**Задание.** Придумайте и проверьте с помощью метода `doesnt_match` несколько последовательностей с лишними словами"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "16c7mAZ_kd68"
      },
      "source": [
        "### Как обучить свою модель\n",
        "\n",
        "В качестве обучающих данных возьмем размеченные и неразмеченные отзывы о фильмах (датасет взят с Kaggle)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EX66ryM0klX5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a60ffb72-d6c0-4845-d46d-2fa3d0d7b958"
      },
      "source": [
        "# скачиваем датасет\n",
        "! wget https://raw.githubusercontent.com/ancatmara/data-science-nlp/master/data/w2v/train/unlabeledTrainData.tsv"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-11-09 11:40:40--  https://raw.githubusercontent.com/ancatmara/data-science-nlp/master/data/w2v/train/unlabeledTrainData.tsv\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 67281491 (64M) [text/plain]\n",
            "Saving to: ‘unlabeledTrainData.tsv’\n",
            "\n",
            "unlabeledTrainData. 100%[===================>]  64.16M  78.2MB/s    in 0.8s    \n",
            "\n",
            "2020-11-09 11:40:44 (78.2 MB/s) - ‘unlabeledTrainData.tsv’ saved [67281491/67281491]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-gLAXFcvkiOM"
      },
      "source": [
        "Загрузим датасет в датафрейм и посмотрим на него, делаем это с помощью уже привычной библиотеки **pandas**:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "okHBivCVksUW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "286a1fa2-2c5e-4679-8202-50998b486042"
      },
      "source": [
        "# считываем данные в формате csv\n",
        "data = pd.read_csv(\"unlabeledTrainData.tsv\", header=0, delimiter=\"\\t\", quoting=3)\n",
        "\n",
        "len(data)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "50000"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 84
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qHdjpqcCkyT1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 195
        },
        "outputId": "0062bf2c-4f80-4750-af8e-d21ebe2f3a1e"
      },
      "source": [
        "data.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>review</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>\"9999_0\"</td>\n",
              "      <td>\"Watching Time Chasers, it obvious that it was...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>\"45057_0\"</td>\n",
              "      <td>\"I saw this film about 20 years ago and rememb...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>\"15561_0\"</td>\n",
              "      <td>\"Minor Spoilers&lt;br /&gt;&lt;br /&gt;In New York, Joan B...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>\"7161_0\"</td>\n",
              "      <td>\"I went to see this film with a great deal of ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>\"43971_0\"</td>\n",
              "      <td>\"Yes, I agree with everyone on this site this ...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "          id                                             review\n",
              "0   \"9999_0\"  \"Watching Time Chasers, it obvious that it was...\n",
              "1  \"45057_0\"  \"I saw this film about 20 years ago and rememb...\n",
              "2  \"15561_0\"  \"Minor Spoilers<br /><br />In New York, Joan B...\n",
              "3   \"7161_0\"  \"I went to see this film with a great deal of ...\n",
              "4  \"43971_0\"  \"Yes, I agree with everyone on this site this ..."
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 86
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "im53Mp3yk1ld",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 142
        },
        "outputId": "a46f901c-ec06-4c40-eccf-d0cba0b2170d"
      },
      "source": [
        "data.iloc[10]['review']"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'\"After reading the comments for this movie, I am not sure whether I should be angry, sad or sickened. Seeing comments typical of people who a)know absolutely nothing about the military or b)who base everything they think they know on movies like this or on CNN reports about Abu-Gharib makes me wonder about the state of intellectual stimulation in the world.<br /><br />At the time I type this the number of people in the US military: 1.4 million on Active Duty with another almost 900,000 in the Guard and Reserves for a total of roughly 2.3 million.<br /><br />The number of people indicted for abuses at at Abu-Gharib: Currently less than 20<br /><br />That makes the total of people indicted .00083% of the total military. Even if you indict every single military member that ever stepped in to Abu-Gharib, you would not come close to making that a whole number. <br /><br />The flaws in this movie would take YEARS to cover. I understand that it\\'s supposed to be sarcastic, but in reality, the writer and director are trying to make commentary about the state of the military without an enemy to fight. In reality, the US military has been at its busiest when there are not conflicts going on. The military is the first called for disaster relief and humanitarian aid missions. When the tsunami hit Indonesia, devestating the region, the US military was the first on the scene. When the chaos of the situation overwhelmed the local governments, it was military leadership who looked at their people, the same people this movie mocks, and said make it happen. Within hours, food aid was reaching isolated villages. Within days, airfields were built, cargo aircraft started landing and a food distribution system was up and running. Hours and days, not weeks and months. Yes there are unscrupulous people in the US military. But then, there are in every walk of life, every occupation. But to see people on this website decide that 2.3 million men and women are all criminal, with nothing on their minds but thoughts of destruction or mayhem is an absolute disservice to the things that they do every day. One person on this website even went so far as to say that military members are in it for personal gain. Wow! Entry level personnel make just under $8.00 an hour assuming a 40 hour work week. Of course, many work much more than 40 hours a week and those in harm\\'s way typically put in 16-18 hour days for months on end. That makes the pay well under minimum wage. So much for personal gain. I beg you, please make yourself familiar with the world around you. Go to a nearby base, get a visitor pass and meet some of the men and women you are so quick to disparage. You would be surprised. The military no longer accepts people in lieu of prison time. They require a minimum of a GED and prefer a high school diploma. The middle ranks are expected to get a minimum of undergraduate degrees and the upper ranks are encouraged to get advanced degrees.\"'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 87
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "knptEXQtkhW7"
      },
      "source": [
        "Нам необходимо отчистить данные от лишнего: убрать ссылки, html-разметку и небуквенные символы. Затем нужно привести все к нижнему регистру и токенизировать. \n",
        "\n",
        "На выходе мы хотим получить массив из предложений, каждое из которых представляет собой массив слов.\n",
        "\n",
        "Импортируем необходимые библиотеки и методы (некоторые уже были испортированы ранее, но для полноты картины оставим их):"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G0mEEpUmk6J6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "951bcacf-340f-46cc-a340-c4dc64ba153f"
      },
      "source": [
        "import nltk.data # библиотека Natural Language Toolkit\n",
        "import re   # библиотека для регулярных выражений\n",
        "from bs4 import BeautifulSoup # библиотека для парсинга xml\n",
        "from nltk.corpus import stopwords # стоп-слова из NLTK\n",
        "from nltk.tokenize import sent_tokenize, RegexpTokenizer  # токенизаторы из NLTK\n",
        "nltk.download('punkt') # пунктуация для правильной работы токенизатора"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 88
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nhz3Olb4lDvz"
      },
      "source": [
        "tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0yhts6MulJaD"
      },
      "source": [
        "Функции для очистки данных:\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_veEBUBZlKN6"
      },
      "source": [
        "def review_to_wordlist(review, remove_stopwords=False):\n",
        "    # убираем ссылки вне тегов\n",
        "    review = re.sub(r\"http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+\", \" \", review)\n",
        "    # достаем сам текст\n",
        "    review_text = BeautifulSoup(review, \"lxml\").get_text()\n",
        "    # оставляем только буквенные символы\n",
        "    review_text = re.sub(\"[^a-zA-Z]\",\" \", review_text)\n",
        "    # приводим к нижнему регистру и разбиваем на слова по символу пробела\n",
        "    words = review_text.lower().split()\n",
        "    if remove_stopwords:\n",
        "      # убираем стоп-слова\n",
        "        stops = stopwords.words(\"english\")\n",
        "        words = [w for w in words if not w in stops]\n",
        "    return(words)\n",
        "\n",
        "def review_to_sentences(review, tokenizer, remove_stopwords=False):\n",
        "    raw_sentences = tokenizer.tokenize(review.strip())\n",
        "    sentences = []\n",
        "    for raw_sentence in raw_sentences:\n",
        "        if len(raw_sentence) > 0:\n",
        "            sentences.append(review_to_wordlist(raw_sentence, remove_stopwords))\n",
        "    return sentences"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qfEIEFIJlTBH"
      },
      "source": [
        "Проходим по всему датасету и парсим написанной выше функцией  текст в списки слов, удаляя при этом лишнее:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ujigc0V2lWmw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bb67d4fb-332d-43f7-b665-add63c711a7b"
      },
      "source": [
        "sentences = []  \n",
        "\n",
        "print(\"Parsing sentences from training set...\")\n",
        "for review in data[\"review\"]:\n",
        "    sentences += review_to_sentences(review, tokenizer)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Parsing sentences from training set...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/bs4/__init__.py:273: UserWarning: \"b'.'\" looks like a filename, not markup. You should probably open this file and pass the filehandle into Beautiful Soup.\n",
            "  ' Beautiful Soup.' % markup)\n",
            "/usr/local/lib/python3.6/dist-packages/bs4/__init__.py:273: UserWarning: \"b'..'\" looks like a filename, not markup. You should probably open this file and pass the filehandle into Beautiful Soup.\n",
            "  ' Beautiful Soup.' % markup)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fYePh1_GlVH5"
      },
      "source": [
        "Посмотрим, что получилось:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RYsO6Ya7lanY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d2d8ccff-e64f-4929-e635-5f103acdffdc"
      },
      "source": [
        "print(len(sentences))\n",
        "print(sentences[0])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "528987\n",
            "['watching', 'time', 'chasers', 'it', 'obvious', 'that', 'it', 'was', 'made', 'by', 'a', 'bunch', 'of', 'friends']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xNp27TyyljLr"
      },
      "source": [
        "# это понадобится нам позже для обучения другой модели эмбеддингов \n",
        "\n",
        "with open('clean_text.txt', 'w') as f:\n",
        "    for s in sentences[:5000]:\n",
        "        f.write(' '.join(s))\n",
        "        f.write('\\n')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fWVpmrVMlm0u"
      },
      "source": [
        "Обучаем и сохраняем модель. \n",
        "\n",
        "\n",
        "Основные параметры:\n",
        "* данные должны быть итерируемым объектом \n",
        "* size — размер вектора, \n",
        "* window — размер окна наблюдения,\n",
        "* min_count — мин. частотность слова в корпусе,\n",
        "* sg — используемый алгоритм обучения (0 — CBOW, 1 — Skip-gram),\n",
        "* sample — порог для downsampling'a высокочастотных слов,\n",
        "* workers — количество потоков,\n",
        "* alpha — learning rate,\n",
        "* iter — количество итераций,\n",
        "* max_vocab_size — позволяет выставить ограничение по памяти при создании словаря (т.е. если ограничение превышается, то низкочастотные слова будут выбрасываться). Для сравнения: 10 млн слов = 1Гб RAM.\n",
        "\n",
        "**NB!** Обратите внимание, что тренировка модели не включает препроцессинг! Это значит, что избавляться от пунктуации, приводить слова к нижнему регистру, лемматизировать их, проставлять частеречные теги придется до тренировки модели (если, конечно, это необходимо для вашей задачи). Т.е. в каком виде слова будут в исходном тексте, в таком они будут и в модели."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TEECgAnQlppC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b86675ff-0ff8-4dc0-afd8-05cf99c5cdfc"
      },
      "source": [
        "print(\"Training model...\")\n",
        "# обучаем модель с векторами размерности 300, длиной окна 10\n",
        "%time model_en = word2vec.Word2Vec(sentences, workers=4, size=300, min_count=10, window=10, sample=1e-3)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training model...\n",
            "CPU times: user 4min 32s, sys: 967 ms, total: 4min 33s\n",
            "Wall time: 2min 24s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7kIOEvpIlt3K"
      },
      "source": [
        "Смотрим, сколько в модели слов."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nM8Cpervlv4_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e4ed09d7-b6a6-415a-c9e8-8fdba8776285"
      },
      "source": [
        "print(len(model_en.wv.vocab))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "28308\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jvSW75bbl5OB"
      },
      "source": [
        "Попробуем оценить модель вручную, порешав примеры. Несколько дано ниже, попробуйте придумать свои."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wpOa55MXl0br",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dfb04460-d947-4324-e0b5-7ef8222d29da"
      },
      "source": [
        "print(model_en.wv.most_similar(positive=[\"woman\", \"actor\"], negative=[\"man\"], topn=1))\n",
        "print(model_en.wv.most_similar(positive=[\"dogs\", \"man\"], negative=[\"dog\"], topn=1))\n",
        "\n",
        "print(model_en.wv.most_similar(\"usa\", topn=3))\n",
        "\n",
        "print(model_en.wv.doesnt_match(\"comedy thriller western novel\".split()))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[('actress', 0.7793270945549011)]\n",
            "[('men', 0.6503796577453613)]\n",
            "[('europe', 0.7524359226226807), ('germany', 0.7389851808547974), ('north', 0.7247494459152222)]\n",
            "novel\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/gensim/matutils.py:737: FutureWarning: Conversion of the second argument of issubdtype from `int` to `np.signedinteger` is deprecated. In future, it will be treated as `np.int64 == np.dtype(int).type`.\n",
            "  if np.issubdtype(vec.dtype, np.int):\n",
            "/usr/local/lib/python3.6/dist-packages/gensim/models/keyedvectors.py:895: FutureWarning: arrays to stack must be passed as a \"sequence\" type such as list or tuple. Support for non-sequence iterables such as generators is deprecated as of NumPy 1.16 and will raise an error in the future.\n",
            "  vectors = vstack(self.word_vec(word, use_norm=True) for word in used_words).astype(REAL)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CJoBb8olmGYQ"
      },
      "source": [
        "### Как дообучить существующую модель\n",
        "\n",
        "При тренировке модели \"с нуля\" веса инициализируются случайно, однако, можно использовать для инициализации векторов веса из предобученной модели, таким образом как бы дообучая ее.\n",
        "\n",
        "Сначала посмотрим близость какой-нибудь пары слов в имеющейся модели, чтобы потом сравнить результат с дообученной."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "171hCwa6mJiz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "37f7d73a-daea-4b0b-a537-5c53916096d5"
      },
      "source": [
        "model_en.wv.similarity('lion', 'rabbit')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/gensim/matutils.py:737: FutureWarning: Conversion of the second argument of issubdtype from `int` to `np.signedinteger` is deprecated. In future, it will be treated as `np.int64 == np.dtype(int).type`.\n",
            "  if np.issubdtype(vec.dtype, np.int):\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.287468"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 173
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sYpQqzcamNL1"
      },
      "source": [
        "В качестве дополнительных данных для обучения возьмем английский текст «Алисы в Зазеркалье»."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YMEwE7OimPe9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "944308c5-73bc-4576-e860-998304c715cb"
      },
      "source": [
        "! wget https://raw.githubusercontent.com/ancatmara/data-science-nlp/master/data/w2v/train/alice.txt"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-11-07 07:27:28--  https://raw.githubusercontent.com/ancatmara/data-science-nlp/master/data/w2v/train/alice.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 167631 (164K) [text/plain]\n",
            "Saving to: ‘alice.txt’\n",
            "\n",
            "alice.txt           100%[===================>] 163.70K  --.-KB/s    in 0.04s   \n",
            "\n",
            "2020-11-07 07:27:28 (3.80 MB/s) - ‘alice.txt’ saved [167631/167631]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sV2bGBFTmT0z",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "14701b4d-58b9-4b64-b9e4-41e0bc94f6c1"
      },
      "source": [
        "with open(\"alice.txt\", 'r', encoding='utf-8') as f:\n",
        "    text = f.read()\n",
        "\n",
        "# убираем переносы строк, токенизируем текст\n",
        "\n",
        "text = re.sub('\\n', ' ', text)\n",
        "sents = sent_tokenize(text)\n",
        "\n",
        "punct = '!\"#$%&()*+,-./:;<=>?@[\\]^_`{|}~„“«»†*—/\\-‘’'\n",
        "clean_sents = []\n",
        "\n",
        "# убираем всю пунктуацию и делим текст на слова по пробелу\n",
        "for sent in sents:\n",
        "    s = [w.lower().strip(punct) for w in sent.split()]\n",
        "    clean_sents.append(s)\n",
        "    \n",
        "print(clean_sents[:2])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[['through', 'the', 'looking-glass', 'by', 'lewis', 'carroll', 'chapter', 'i', 'looking-glass', 'house', 'one', 'thing', 'was', 'certain', 'that', 'the', 'white', 'kitten', 'had', 'had', 'nothing', 'to', 'do', 'with', 'it', '', 'it', 'was', 'the', 'black', 'kitten’s', 'fault', 'entirely'], ['for', 'the', 'white', 'kitten', 'had', 'been', 'having', 'its', 'face', 'washed', 'by', 'the', 'old', 'cat', 'for', 'the', 'last', 'quarter', 'of', 'an', 'hour', 'and', 'bearing', 'it', 'pretty', 'well', 'considering', 'so', 'you', 'see', 'that', 'it', 'couldn’t', 'have', 'had', 'any', 'hand', 'in', 'the', 'mischief']]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T2O6QnwcmZYo"
      },
      "source": [
        "Чтобы дообучить модель, надо сначала ее сохранить, а потом загрузить. Все параметры тренировки (размер вектора, мин. частота слова и т.п.) будут взяты из загруженной модели, т.е. задать их заново нельзя.\n",
        "\n",
        "**NB!** Дообучить можно только полную модель (сохраненные при обучении веса и параметры модели, то есть обект самой модели), а `KeyedVectors` (просто пары \"слово - вектор\") — нельзя. Поэтому сохранять модель нужно в соотвествующем формате. Подробнее о разнице [вот тут](https://radimrehurek.com/gensim/models/keyedvectors.html)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-z28eER7mc-Y",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "77b6e2c1-d7b8-40e8-aa37-5b121cf338e5"
      },
      "source": [
        "model_path = \"movie_reviews.model\"\n",
        "\n",
        "# так можно сохранить модель для последующего дообучения\n",
        "print(\"Saving model...\")\n",
        "model_en.save(model_path)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Saving model...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a-i0jCyGme7C",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "67fed096-e891-49da-84c7-dfe250b5326b"
      },
      "source": [
        "# загружаем нашу обученную модель и дообучаем на текстах \"Алисы\"\n",
        "\n",
        "model = word2vec.Word2Vec.load(model_path)\n",
        "\n",
        "model.build_vocab(clean_sents, update=True)\n",
        "model.train(clean_sents, total_examples=model.corpus_count, epochs=5)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(96730, 150225)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 178
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_pY1ftSImhv1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "82c75f15-66a2-4b91-c1fe-2acb9d98f9f2"
      },
      "source": [
        "model.wv.similarity('lion', 'rabbit')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/gensim/matutils.py:737: FutureWarning: Conversion of the second argument of issubdtype from `int` to `np.signedinteger` is deprecated. In future, it will be treated as `np.int64 == np.dtype(int).type`.\n",
            "  if np.issubdtype(vec.dtype, np.int):\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.30270875"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 179
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b02DdZigmkWw"
      },
      "source": [
        "Лев и кролик стали ближе друг к другу!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eLOxX4yUmsPU"
      },
      "source": [
        "Можно нормализовать вектора, тогда модель будет занимать меньше RAM. Однако после этого её нельзя дотренировывать. Здесь используется L2-нормализация: вектора нормализуются так, что если сложить квадраты всех элементов вектора, в сумме получится 1. \n",
        "\n",
        "Кроме того, сохраним не полные вектора, а `KeyedVectors`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eyObwn1GmuuS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6133794f-da15-44db-e318-311c4ff5a6a6"
      },
      "source": [
        "model.init_sims(replace=True)\n",
        "model_path = \"movies_alice.bin\"\n",
        "\n",
        "print(\"Saving model...\")\n",
        "model_en.wv.save_word2vec_format(model_path, binary=True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Saving model...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IAgKkUuVnD83"
      },
      "source": [
        "## Оценка\n",
        "\n",
        "Задача обучения модели w2v - это usupervised задача (обучение без учителя), \"правильных\" ответов нет, поэтому нельзя вычислить некую метрику качества, чтобы сравнить две модели между собой или просто по значению одной метрики сказать, насколько хороша полученная модель. \n",
        "\n",
        "Тем не менее, существуют специальные выборки для оценки качества дистрибутивных моделей. Основных два: один измеряет точность решения задач на аналогии (пример про Россию и пельмени), а второй используется для оценки коэффициента семантической близости. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VTpG9KjE9eUQ"
      },
      "source": [
        "### Аналогии\n",
        "\n",
        "Другая популярная задача для \"внутренней\" оценки называется задачей поиска аналогий. Как мы уже разбирали выше, с помощью простых арифметических операций мы можем модифицировать значение слова. Если заранее собрать набор слов-модификаторов, а также слов, которые мы хотим получить в результаты модификации, то на основе подсчёта количества \"попаданий\" в желаемое слово мы можем оценить, насколько хорошо работает модель.\n",
        "\n",
        "В качестве слов-модификаторов мы можем использовать семантические аналогии. Скажем, если у нас есть некоторое отношение \"страна-столица\", то для оценки модели мы можем использовать пары наподобие \"Россия-Москва\", \"Норвегия-Осло\", и т.д. Выборка будет выглядеть следующм образом:\n",
        "\n",
        "| слово 1    | слово 2    | отношение     | \n",
        "|------------|------------|---------------|\n",
        "| Россия     | Москва     | страна-столица|  \n",
        "| Норвегия   | Осло       | страна-столица|\n",
        "\n",
        "Рассматривая случайные две пары из этого набора, мы хотим, имея триплет (Россия, Москва, Норвегия), получить слово \"Осло\", т.е. найти такое слово, которое будет находиться в том же отношении со словом \"Норвегия\", как \"Россия\" находится с Москвой. \n",
        "\n",
        "Выборки для русского языка можно скачать на странице с моделями на RusVectores. Посчитаем качество нашей модели НКРЯ на выборке про аналогии:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FxRUo3g_9hw0"
      },
      "source": [
        "! wget https://raw.githubusercontent.com/ancatmara/data-science-nlp/master/data/w2v/evaluation/ru_analogy_tagged.txt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yUJHg-6v9lF6"
      },
      "source": [
        "with open('ru_analogy_tagged.txt','r') as f:\n",
        "  data = f.readlines()\n",
        "  print (data[:10])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lAHA6zn09lrP"
      },
      "source": [
        "res = model_ru.accuracy('ru_analogy_tagged.txt')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bdf-h9Uf9r5N"
      },
      "source": [
        "print(res[4]['incorrect'][:10])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pBJ0_n2e9yct"
      },
      "source": [
        "### Word Similarity\n",
        "\n",
        "Этот метод заключается в том, чтобы оценить, насколько представления о семантической близости слов в модели соотносятся с \"представлениями\" людей.\n",
        "\n",
        "| слово 1    | слово 2    | близость | \n",
        "|------------|------------|----------|\n",
        "| кошка      | собака     | 0.7      |  \n",
        "| чашка      | кружка     | 0.9      |       \n",
        "\n",
        "Для каждой пары слов из заранее заданного датасета мы можем посчитать косинусное расстояние, и получить список таких значений близости. При этом у нас уже есть список значений близостей, сделанный людьми. Мы можем сравнить эти два списка и понять, насколько они похожи (например, посчитав корреляцию). Эта мера схожести должна говорить о том, насколько модель хорошо моделирует расстояния до слова.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AsnuE_CgnJO3"
      },
      "source": [
        "## Бонус 3. FastText\n",
        "\n",
        "FastText использует не только эмбеддинги слов, но и эмбеддинги n-грам. В корпусе каждое слово автоматически представляется в виде набора символьных n-грамм. \n",
        "\n",
        "Скажем, если мы установим n=3, то вектор для слова \"where\" будет представлен суммой векторов следующих триграм: \"<wh\", \"whe\", \"her\", \"ere\", \"re>\" (где \"<\" и \">\" символы, обозначающие начало и конец слова). \n",
        "\n",
        "Благодаря этому мы можем также получать вектора для слов, отсутствуюших в словаре, а также эффективно работать с текстами, содержащими ошибки и опечатки.\n",
        "\n",
        "* [Статья](https://aclweb.org/anthology/Q17-1010)\n",
        "* [Сайт](https://fasttext.cc/)\n",
        "* [Тьюториал](https://fasttext.cc/docs/en/support.html)\n",
        "* [Вектора для 157 языков](https://fasttext.cc/docs/en/crawl-vectors.html)\n",
        "* [Вектора, обученные на википедии](https://fasttext.cc/docs/en/pretrained-vectors.html) (отдельно для 294 разных языков)\n",
        "* [Репозиторий](https://github.com/facebookresearch/fasttext)\n",
        "\n",
        "Есть библиотека `fasttext` для питона (с готовыми моделями можно работать и через `gensim`)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-UXvgavDnLlx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5437b6d3-3d8a-4829-d347-3bf8358b1e60"
      },
      "source": [
        "# чтобы установить fasstext, можно склонировать его с репозитория \n",
        "! git clone https://github.com/facebookresearch/fastText.git\n",
        "! pip3 install fastText/."
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'fastText'...\n",
            "remote: Enumerating objects: 3854, done.\u001b[K\n",
            "remote: Total 3854 (delta 0), reused 0 (delta 0), pack-reused 3854\u001b[K\n",
            "Receiving objects: 100% (3854/3854), 8.22 MiB | 4.26 MiB/s, done.\n",
            "Resolving deltas: 100% (2417/2417), done.\n",
            "Processing ./fastText\n",
            "Requirement already satisfied: pybind11>=2.2 in /usr/local/lib/python3.6/dist-packages (from fasttext==0.9.2) (2.6.0)\n",
            "Requirement already satisfied: setuptools>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from fasttext==0.9.2) (50.3.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from fasttext==0.9.2) (1.18.5)\n",
            "Building wheels for collected packages: fasttext\n",
            "  Building wheel for fasttext (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fasttext: filename=fasttext-0.9.2-cp36-cp36m-linux_x86_64.whl size=3015969 sha256=328c8abc01d195aada95fc6cfe58bde6309cc19b6c0dba33982cc90883066928\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-blkc9vqw/wheels/a1/9f/52/696ce6c5c46325e840c76614ee5051458c0df10306987e7443\n",
            "Successfully built fasttext\n",
            "Installing collected packages: fasttext\n",
            "Successfully installed fasttext-0.9.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GerzfRvLnQfC"
      },
      "source": [
        "import fasttext\n",
        "\n",
        "ft_model = fasttext.train_unsupervised('clean_text.txt', minn=3, maxn=4, dim=300)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fL0w3IrUnTr_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "89fda736-a756-4e7b-8f99-b30200abe0f7"
      },
      "source": [
        "ft_model.get_nearest_neighbors('actor')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(0.9999606013298035, 'actors'),\n",
              " (0.9999364018440247, 'attractive'),\n",
              " (0.9999338984489441, 'fact'),\n",
              " (0.9999316334724426, 'actual'),\n",
              " (0.9999226331710815, 'display'),\n",
              " (0.9999191761016846, 'terrific'),\n",
              " (0.9999188780784607, 'battle'),\n",
              " (0.9999170899391174, 'israel'),\n",
              " (0.9999163746833801, 'british'),\n",
              " (0.9999160170555115, 'predator')]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 183
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Akwp4geNnWMh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9076c901-3234-4817-e978-1451d127dcbd"
      },
      "source": [
        "ft_model.get_analogies(\"woman\", \"man\", \"actor\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(0.999938428401947, 'act'),\n",
              " (0.9998956918716431, 'exactly'),\n",
              " (0.9998955726623535, 'actress'),\n",
              " (0.999885082244873, 'seemingly'),\n",
              " (0.9998830556869507, 'terrible'),\n",
              " (0.9998824596405029, 'surprisingly'),\n",
              " (0.9998821020126343, 'believable'),\n",
              " (0.9998811483383179, 'double'),\n",
              " (0.9998807907104492, 'written'),\n",
              " (0.9998795986175537, 'cable')]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 100
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J_yBTNiTnbQj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fec77e7d-207a-4c16-df68-e739528a6554"
      },
      "source": [
        "ft_model.get_nearest_neighbors('actr')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(0.9999391436576843, 'act'),\n",
              " (0.9998903274536133, 'actors'),\n",
              " (0.9998863339424133, 'actor'),\n",
              " (0.9998792409896851, 'actress'),\n",
              " (0.9998623728752136, 'single'),\n",
              " (0.9998517632484436, 'actual'),\n",
              " (0.9998226761817932, 'terrible'),\n",
              " (0.9998196363449097, 'exact'),\n",
              " (0.9998190402984619, 'plot'),\n",
              " (0.9998172521591187, 'wrong')]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 101
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lyk-qki4njoE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8556f6df-4bbe-4cfd-de5e-0697ea21010c"
      },
      "source": [
        "ft_model.get_nearest_neighbors('moviegeek')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(0.9999324679374695, 'reviews'),\n",
              " (0.9999246597290039, 'review'),\n",
              " (0.9999151825904846, 'recommended'),\n",
              " (0.9999132752418518, 'rented'),\n",
              " (0.9998916387557983, 'waste'),\n",
              " (0.999889075756073, 'movie'),\n",
              " (0.9998835921287537, 'thank'),\n",
              " (0.9998812079429626, 'not'),\n",
              " (0.9998751878738403, 'watchable'),\n",
              " (0.9998645782470703, 'only')]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 102
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "igRgYdQFzU62"
      },
      "source": [
        "Дополнение: https://github.com/dipanjanS/text-analytics-with-python/tree/master/New-Second-Edition"
      ]
    }
  ]
}